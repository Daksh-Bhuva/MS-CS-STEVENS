{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZ1qD7BWCVF"
      },
      "source": [
        "# Assignment 2: Build a CNN for image recognition.\n",
        "\n",
        "## Due Date:  March 29, 11:59PM\n",
        "\n",
        "### Name: [Daksh Bhuva]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ihBZYWWCVG"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "1. In this assignment, you will build Convolutional Neural Network to classify CIFAR-10 Images.\n",
        "2. You can directly load dataset from many deep learning packages.\n",
        "3. You can use any deep learning packages such as pytorch, keras or tensorflow for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibHnyC_WCVH"
      },
      "source": [
        "## Requirements:\n",
        "\n",
        "1. You need to load cifar 10 data and split the entire training dataset into training and validation.\n",
        "2. You will implement a CNN model to classify cifar 10 images with provided structure.\n",
        "3. You need to plot the training and validation accuracy or loss obtained from above step.\n",
        "4. Then you can use tuned parameters to train using the entire training dataset.\n",
        "5. You should report the testing accuracy using the model with complete data.\n",
        "6. You may try to change the structure (e.g, add BN layer or dropout layer,...) and analyze your findings.\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvFm15mvWCVH"
      },
      "source": [
        "## Batch Normalization (BN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgX9WkXWCVH"
      },
      "source": [
        "### Background:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69C2wFYoWCVI"
      },
      "source": [
        "- Batch Normalization is a technique to speed up training and help make the model more stable.\n",
        "- In simple words, batch normalization is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
        "\n",
        "- For more detailed information, you may refer to the original paper: https://arxiv.org/pdf/1502.03167.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKQeXItSWCVI"
      },
      "source": [
        "### BN Algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My9jWvdeWCVI"
      },
      "source": [
        "- Input: Values of $x$ over a mini-batch: $\\mathbf{B}$ = $\\{x_1,..., x_m\\};$\n",
        "- Output: $\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}$, $\\gamma, \\beta$ are learnable parameters\n",
        "\n",
        "Normalization of the Input:\n",
        "$$\\mu_{\\mathbf{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
        "$$\\sigma_{\\mathbf{B}}^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathbf{B}})^2$$\n",
        "$$\\hat{x_i} = \\frac{x_i - \\mu_{\\mathbf{B}}}{\\sqrt{\\sigma_{\\mathbf{B}}}^2 + \\epsilon}$$\n",
        "Re-scaling and Offsetting:\n",
        "$$y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma,\\beta}(x_i)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_t0_MMjWCVJ"
      },
      "source": [
        "### Advantages of BN:\n",
        "1. Improves gradient flow through the network.\n",
        "2. Allows use of saturating nonlinearities and higher learning rates.\n",
        "3. Makes weights easier to initialize.\n",
        "4. Act as a form of regularization and may reduce the need for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9PH3rbdWCVJ"
      },
      "source": [
        "### Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iShLqN1WCVJ"
      },
      "source": [
        "- The batch normalization layer has already been implemented in many packages. You may simply call the function to build the layer. For example: torch.nn.BatchNorm2d() using pytroch package, keras.layers.BatchNormalization() using keras package.\n",
        "- The location of BN layer: Please make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x1NLMcLWCVJ"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yigOUJWCVK"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0wr42JmWCVK",
        "outputId": "87d52702-4408-475a-8bfe-8a6c5b1261f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Load Cifar-10 Data\n",
        "# This is just an example, you may load dataset from other packages.\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "\n",
        "### If you can not load keras dataset, un-comment these two lines.\n",
        "# import ssl\n",
        "# ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvDDHZDIWCVL"
      },
      "source": [
        "### 1.2. One-hot encode the labels (5 points)\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Implement a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKA4wOBdWCVL",
        "outputId": "2c80486f-b951-49b5-d52b-9442d23d8b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "  onehot_encoded = np.zeros((y.size, num_class))\n",
        "  onehot_encoded[np.arange(y.size), y.reshape((1, y.size))] = 1\n",
        "  return onehot_encoded\n",
        "  pass\n",
        "\n",
        "x_train, x_test = x_train.astype('float32') / 255, x_test.astype('float32') / 255\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArjscD5AWCVL"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO3Cn_MFWCVL"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets (5 points)\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets: \n",
        "* a training set containing 40K samples: x_tr, y_tr\n",
        "* a validation set containing 10K samples: x_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jffNUbPWCVL",
        "outputId": "5219aab2-1d2f-4ca3-f3f6-3186b0a33c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "rand_indices = np.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIhEl9dWCVL"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters (50 points)\n",
        "\n",
        "- Build a convolutional neural network model using the below structure:\n",
        "\n",
        "- It should have a structure of: Conv - ReLU - Max Pool - ConV - ReLU - Max Pool - Dense - ReLU - Dense - Softmax\n",
        "\n",
        "- In the graph 3@32x32 means the dimension of input image, 32@30x30 means it has 32 filters and the dimension now becomes 30x30 after the convolution.\n",
        "- All convolutional layers (Conv) should have stride = 1 and no padding.\n",
        "- Max Pooling has a pool size of 2 by 2.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeXXZv5kWCVL"
      },
      "source": [
        "<img src=\"/content/network.PNG\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnfxyeV3WCVL"
      },
      "source": [
        "- You may use the validation data to tune the hyper-parameters (e.g., learning rate, and optimization algorithm)\n",
        "- Do NOT use test data for hyper-parameter tuning!!!\n",
        "- Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxzBKX7lC3S"
      },
      "source": [
        "## Original Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FNLG75QWCVM",
        "outputId": "c0cf6351-1783-4dbd-ddd5-57bb0fbb7035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,378\n",
            "Trainable params: 626,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4), activation = 'relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sQuhHwYWCVM"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = 0.0001) , metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWFGioHoWCVM",
        "outputId": "f13148f8-def7-4584-b691-6b40260b1333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 20s 10ms/step - loss: 1.7683 - acc: 0.3729 - val_loss: 1.5341 - val_acc: 0.4584\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 1.4608 - acc: 0.4847 - val_loss: 1.3600 - val_acc: 0.5209\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3324 - acc: 0.5334 - val_loss: 1.3018 - val_acc: 0.5297\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2465 - acc: 0.5688 - val_loss: 1.2122 - val_acc: 0.5703\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.1808 - acc: 0.5912 - val_loss: 1.1587 - val_acc: 0.5915\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 1.1248 - acc: 0.6108 - val_loss: 1.1258 - val_acc: 0.6009\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.0793 - acc: 0.6252 - val_loss: 1.1044 - val_acc: 0.6097\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.0393 - acc: 0.6430 - val_loss: 1.0572 - val_acc: 0.6263\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 1.0037 - acc: 0.6549 - val_loss: 1.0546 - val_acc: 0.6261\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.9733 - acc: 0.6643 - val_loss: 1.0491 - val_acc: 0.6327\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9452 - acc: 0.6747 - val_loss: 1.0102 - val_acc: 0.6467\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.9161 - acc: 0.6825 - val_loss: 0.9863 - val_acc: 0.6512\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.8903 - acc: 0.6951 - val_loss: 0.9989 - val_acc: 0.6487\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8660 - acc: 0.7022 - val_loss: 0.9411 - val_acc: 0.6700\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8438 - acc: 0.7126 - val_loss: 0.9626 - val_acc: 0.6644\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8198 - acc: 0.7192 - val_loss: 0.9236 - val_acc: 0.6846\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7960 - acc: 0.7290 - val_loss: 0.9333 - val_acc: 0.6758\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7740 - acc: 0.7342 - val_loss: 0.9656 - val_acc: 0.6693\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7536 - acc: 0.7414 - val_loss: 0.9067 - val_acc: 0.6885\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7338 - acc: 0.7510 - val_loss: 0.9327 - val_acc: 0.6790\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7128 - acc: 0.7564 - val_loss: 0.9229 - val_acc: 0.6823\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6917 - acc: 0.7642 - val_loss: 0.8799 - val_acc: 0.6988\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6730 - acc: 0.7728 - val_loss: 0.8998 - val_acc: 0.6932\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6557 - acc: 0.7777 - val_loss: 0.9224 - val_acc: 0.6830\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6364 - acc: 0.7839 - val_loss: 0.8770 - val_acc: 0.7043\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6157 - acc: 0.7926 - val_loss: 0.8873 - val_acc: 0.7043\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5980 - acc: 0.7966 - val_loss: 0.8814 - val_acc: 0.7037\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5794 - acc: 0.8037 - val_loss: 0.9760 - val_acc: 0.6765\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5616 - acc: 0.8117 - val_loss: 0.8789 - val_acc: 0.7075\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5445 - acc: 0.8165 - val_loss: 0.9354 - val_acc: 0.6912\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5262 - acc: 0.8257 - val_loss: 0.9099 - val_acc: 0.6962\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 10s 9ms/step - loss: 0.5079 - acc: 0.8298 - val_loss: 0.9176 - val_acc: 0.7027\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4902 - acc: 0.8375 - val_loss: 0.9152 - val_acc: 0.7024\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4725 - acc: 0.8415 - val_loss: 0.9208 - val_acc: 0.6982\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4570 - acc: 0.8480 - val_loss: 0.9254 - val_acc: 0.7052\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4390 - acc: 0.8558 - val_loss: 0.9082 - val_acc: 0.7119\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4222 - acc: 0.8615 - val_loss: 0.9198 - val_acc: 0.7086\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4068 - acc: 0.8667 - val_loss: 0.9379 - val_acc: 0.7068\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3902 - acc: 0.8719 - val_loss: 0.9427 - val_acc: 0.7076\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3754 - acc: 0.8786 - val_loss: 0.9342 - val_acc: 0.7104\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3608 - acc: 0.8827 - val_loss: 0.9586 - val_acc: 0.7068\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.3442 - acc: 0.8895 - val_loss: 0.9783 - val_acc: 0.7067\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.3299 - acc: 0.8946 - val_loss: 0.9796 - val_acc: 0.7050\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3144 - acc: 0.9003 - val_loss: 0.9965 - val_acc: 0.7077\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2993 - acc: 0.9054 - val_loss: 0.9803 - val_acc: 0.7133\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2850 - acc: 0.9105 - val_loss: 1.0210 - val_acc: 0.7060\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2706 - acc: 0.9163 - val_loss: 1.0402 - val_acc: 0.7028\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2574 - acc: 0.9206 - val_loss: 1.0450 - val_acc: 0.7079\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2447 - acc: 0.9259 - val_loss: 1.0436 - val_acc: 0.7081\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2309 - acc: 0.9300 - val_loss: 1.0979 - val_acc: 0.7058\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2187 - acc: 0.9348 - val_loss: 1.0982 - val_acc: 0.7061\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2067 - acc: 0.9392 - val_loss: 1.1534 - val_acc: 0.7015\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1939 - acc: 0.9437 - val_loss: 1.1094 - val_acc: 0.7121\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1825 - acc: 0.9485 - val_loss: 1.1833 - val_acc: 0.7030\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1726 - acc: 0.9510 - val_loss: 1.1822 - val_acc: 0.7031\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1625 - acc: 0.9553 - val_loss: 1.1803 - val_acc: 0.7053\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1511 - acc: 0.9591 - val_loss: 1.2457 - val_acc: 0.6953\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1421 - acc: 0.9613 - val_loss: 1.2126 - val_acc: 0.7104\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1315 - acc: 0.9657 - val_loss: 1.2604 - val_acc: 0.7052\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1212 - acc: 0.9681 - val_loss: 1.2503 - val_acc: 0.7077\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1145 - acc: 0.9708 - val_loss: 1.3193 - val_acc: 0.7001\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1054 - acc: 0.9729 - val_loss: 1.3521 - val_acc: 0.6993\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0984 - acc: 0.9755 - val_loss: 1.3459 - val_acc: 0.7067\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0910 - acc: 0.9788 - val_loss: 1.4266 - val_acc: 0.6926\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0839 - acc: 0.9806 - val_loss: 1.3925 - val_acc: 0.7033\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0773 - acc: 0.9827 - val_loss: 1.4157 - val_acc: 0.7026\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0716 - acc: 0.9842 - val_loss: 1.4939 - val_acc: 0.7000\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 10s 9ms/step - loss: 0.0659 - acc: 0.9854 - val_loss: 1.5441 - val_acc: 0.6998\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0611 - acc: 0.9876 - val_loss: 1.5014 - val_acc: 0.7012\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0554 - acc: 0.9893 - val_loss: 1.5673 - val_acc: 0.6988\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0516 - acc: 0.9891 - val_loss: 1.5700 - val_acc: 0.7025\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0470 - acc: 0.9909 - val_loss: 1.6097 - val_acc: 0.7027\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0429 - acc: 0.9916 - val_loss: 1.6743 - val_acc: 0.6989\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0396 - acc: 0.9928 - val_loss: 1.6653 - val_acc: 0.7049\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0365 - acc: 0.9933 - val_loss: 1.7583 - val_acc: 0.6920\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0337 - acc: 0.9944 - val_loss: 1.7552 - val_acc: 0.6981\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0308 - acc: 0.9949 - val_loss: 1.7792 - val_acc: 0.7008\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0289 - acc: 0.9952 - val_loss: 1.8006 - val_acc: 0.7013\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0264 - acc: 0.9955 - val_loss: 1.8772 - val_acc: 0.7008\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0242 - acc: 0.9959 - val_loss: 2.0509 - val_acc: 0.6834\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0222 - acc: 0.9962 - val_loss: 1.8676 - val_acc: 0.7009\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0206 - acc: 0.9967 - val_loss: 1.8878 - val_acc: 0.7025\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0185 - acc: 0.9974 - val_loss: 1.9375 - val_acc: 0.7016\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0179 - acc: 0.9966 - val_loss: 1.9668 - val_acc: 0.6992\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0159 - acc: 0.9974 - val_loss: 2.0741 - val_acc: 0.6924\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0153 - acc: 0.9974 - val_loss: 2.0722 - val_acc: 0.7003\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0145 - acc: 0.9973 - val_loss: 2.0597 - val_acc: 0.7035\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0134 - acc: 0.9978 - val_loss: 2.1036 - val_acc: 0.7005\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0125 - acc: 0.9980 - val_loss: 2.1366 - val_acc: 0.7006\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0119 - acc: 0.9979 - val_loss: 2.1312 - val_acc: 0.7027\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0107 - acc: 0.9985 - val_loss: 2.1963 - val_acc: 0.7017\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0102 - acc: 0.9984 - val_loss: 2.3009 - val_acc: 0.6988\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0097 - acc: 0.9980 - val_loss: 2.4203 - val_acc: 0.6876\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0098 - acc: 0.9980 - val_loss: 2.2773 - val_acc: 0.7013\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0090 - acc: 0.9982 - val_loss: 2.2772 - val_acc: 0.6999\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0090 - acc: 0.9981 - val_loss: 2.3063 - val_acc: 0.7006\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0081 - acc: 0.9984 - val_loss: 2.3292 - val_acc: 0.6975\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0088 - acc: 0.9980 - val_loss: 2.4172 - val_acc: 0.6960\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0071 - acc: 0.9987 - val_loss: 2.5172 - val_acc: 0.6913\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0072 - acc: 0.9985 - val_loss: 2.4572 - val_acc: 0.6980\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_orig = model.fit(x_tr, y_tr, batch_size=40, epochs=100, validation_data=(x_val, y_val))\n",
        "model.save('original_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgI3M_dhWCVM"
      },
      "source": [
        "## 3. Plot the training and validation loss curve versus epochs. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "J6ONkvzmWCVM",
        "outputId": "080df723-6fe8-4cc4-b0d7-648679ee974a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_orig.history['acc']\n",
        "val_acc = history_orig.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYljmUOcWCVM"
      },
      "source": [
        "## 4. Train (again) and evaluate the model (5 points)\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W73oG3FWCVM"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyPO7FtmWCVM"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4), activation = 'relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = 0.0001) , metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboJi14xWCVM",
        "outputId": "b86854bb-7841-44ac-fcca-54e33555616b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.7233 - acc: 0.3901\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.4056 - acc: 0.5018\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.2816 - acc: 0.5486\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1984 - acc: 0.5807\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1293 - acc: 0.6069\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.0736 - acc: 0.6251\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 13s 11ms/step - loss: 1.0270 - acc: 0.6424\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.9852 - acc: 0.6585\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.9502 - acc: 0.6713\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.9191 - acc: 0.6832\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.8902 - acc: 0.6922\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 13s 11ms/step - loss: 0.8620 - acc: 0.7030\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8362 - acc: 0.7105\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.8119 - acc: 0.7210\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7897 - acc: 0.7285\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7665 - acc: 0.7369\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7446 - acc: 0.7432\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7209 - acc: 0.7535\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.7019 - acc: 0.7576\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.6818 - acc: 0.7660\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.6610 - acc: 0.7740\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6408 - acc: 0.7813\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 0.6219 - acc: 0.7882\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.6022 - acc: 0.7930\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5838 - acc: 0.8008\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5664 - acc: 0.8073\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5477 - acc: 0.8149\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5309 - acc: 0.8205\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5100 - acc: 0.8276\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4953 - acc: 0.8332\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4763 - acc: 0.8400\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4610 - acc: 0.8444\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.4431 - acc: 0.8511\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4265 - acc: 0.8579\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4117 - acc: 0.8624\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3937 - acc: 0.8693\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3795 - acc: 0.8731\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3636 - acc: 0.8809\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3472 - acc: 0.8874\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3336 - acc: 0.8916\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.3192 - acc: 0.8964\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3052 - acc: 0.9014\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2910 - acc: 0.9068\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2759 - acc: 0.9138\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2622 - acc: 0.9163\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2504 - acc: 0.9230\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2377 - acc: 0.9261\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2241 - acc: 0.9304\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2108 - acc: 0.9363\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2009 - acc: 0.9397\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1882 - acc: 0.9451\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1764 - acc: 0.9488\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1675 - acc: 0.9513\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1563 - acc: 0.9548\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1463 - acc: 0.9581\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1359 - acc: 0.9620\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1263 - acc: 0.9654\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1178 - acc: 0.9685\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1094 - acc: 0.9710\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1023 - acc: 0.9739\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0943 - acc: 0.9768\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0874 - acc: 0.9779\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0803 - acc: 0.9804\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0738 - acc: 0.9826\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0687 - acc: 0.9839\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0636 - acc: 0.9857\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0579 - acc: 0.9870\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0534 - acc: 0.9886\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0487 - acc: 0.9898\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0452 - acc: 0.9907\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0403 - acc: 0.9924\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0384 - acc: 0.9922\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0340 - acc: 0.9940\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0315 - acc: 0.9943\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0295 - acc: 0.9942\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0273 - acc: 0.9946\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0247 - acc: 0.9958\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0242 - acc: 0.9954\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0219 - acc: 0.9958\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0200 - acc: 0.9963\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0184 - acc: 0.9967\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0182 - acc: 0.9963\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0158 - acc: 0.9975\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0153 - acc: 0.9974\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0142 - acc: 0.9975\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0130 - acc: 0.9977\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0123 - acc: 0.9978\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0114 - acc: 0.9980\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0114 - acc: 0.9976\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0108 - acc: 0.9979\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0093 - acc: 0.9984\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0100 - acc: 0.9980\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0096 - acc: 0.9981\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0091 - acc: 0.9980\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0081 - acc: 0.9985\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0080 - acc: 0.9984\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0079 - acc: 0.9983\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0073 - acc: 0.9984\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0073 - acc: 0.9985\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0066 - acc: 0.9988\n"
          ]
        }
      ],
      "source": [
        "#<Train your model on the entire training set (50K samples)>\n",
        "\n",
        "history_final = model.fit(x_train, y_train_vec, batch_size=40, epochs=100)\n",
        "model.save('final_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxY36XYmWCVM"
      },
      "source": [
        "## 5. Evaluate the model on the test set (5 points)\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAdO2M1WCVM",
        "outputId": "eacb8507-e011-4892-d3f7-152c5b6a11d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.4996 - acc: 0.6904\n",
            "loss = 2.499619960784912\n",
            "accuracy = 0.6904000043869019\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('original_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhfs40D7yo5M",
        "outputId": "f440d425-133b-42f5-c0f6-42f73d9bc18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.5541 - acc: 0.7035\n",
            "loss = 2.5541446208953857\n",
            "accuracy = 0.703499972820282\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('final_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUjql8w_WCVM"
      },
      "source": [
        "## 6. Building model with new structure (25 points)\n",
        "- In this section, you can build your model with adding new layers (e.g, BN layer or dropout layer, ...).\n",
        "- If you want to regularize a ```Conv/Dense layer```, you should place a ```Dropout layer``` before the ```Conv/Dense layer```.\n",
        "- You can try to compare their loss curve and testing accuracy and analyze your findings.\n",
        "- You need to try at lease two different model structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8fUnkgMj-jb"
      },
      "source": [
        "## Model 1 (BN + Data_Augmentation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJtXZSyIWCVM",
        "outputId": "6b7ba5c7-d09d-4e61-d6e9-cc2ae0e915b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 627,786\n",
            "Trainable params: 627,082\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPZZb7V3O6Ln"
      },
      "outputs": [],
      "source": [
        "# Using data augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrX-iVdvxYW9"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1E-3), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxgiRMjvP_zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe4aadf-e49b-4b6c-d3bb-89688926aa56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1000/1000 [==============================] - 48s 37ms/step - loss: 1.5709 - acc: 0.4343 - val_loss: 1.8509 - val_acc: 0.4142\n",
            "Epoch 2/150\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 1.3131 - acc: 0.5324 - val_loss: 1.2476 - val_acc: 0.5543\n",
            "Epoch 3/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.2169 - acc: 0.5646 - val_loss: 1.1623 - val_acc: 0.5920\n",
            "Epoch 4/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.1501 - acc: 0.5927 - val_loss: 1.3540 - val_acc: 0.5460\n",
            "Epoch 5/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.1054 - acc: 0.6079 - val_loss: 1.0723 - val_acc: 0.6408\n",
            "Epoch 6/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0624 - acc: 0.6260 - val_loss: 1.1799 - val_acc: 0.6115\n",
            "Epoch 7/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0343 - acc: 0.6365 - val_loss: 0.8989 - val_acc: 0.6852\n",
            "Epoch 8/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0105 - acc: 0.6436 - val_loss: 0.8509 - val_acc: 0.7093\n",
            "Epoch 9/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9897 - acc: 0.6548 - val_loss: 1.0776 - val_acc: 0.6472\n",
            "Epoch 10/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9684 - acc: 0.6599 - val_loss: 0.9053 - val_acc: 0.6832\n",
            "Epoch 11/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9501 - acc: 0.6664 - val_loss: 0.9481 - val_acc: 0.6710\n",
            "Epoch 12/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9333 - acc: 0.6744 - val_loss: 1.2209 - val_acc: 0.6078\n",
            "Epoch 13/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9195 - acc: 0.6829 - val_loss: 1.0200 - val_acc: 0.6802\n",
            "Epoch 14/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9174 - acc: 0.6793 - val_loss: 1.1082 - val_acc: 0.6363\n",
            "Epoch 15/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9027 - acc: 0.6855 - val_loss: 1.3267 - val_acc: 0.5885\n",
            "Epoch 16/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8905 - acc: 0.6904 - val_loss: 1.0966 - val_acc: 0.6663\n",
            "Epoch 17/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8816 - acc: 0.6932 - val_loss: 0.9861 - val_acc: 0.6657\n",
            "Epoch 18/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8758 - acc: 0.6939 - val_loss: 1.3534 - val_acc: 0.6073\n",
            "Epoch 19/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8627 - acc: 0.6995 - val_loss: 1.0055 - val_acc: 0.6875\n",
            "Epoch 20/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8605 - acc: 0.7024 - val_loss: 1.0549 - val_acc: 0.6507\n",
            "Epoch 21/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8452 - acc: 0.7096 - val_loss: 1.1364 - val_acc: 0.6365\n",
            "Epoch 22/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8405 - acc: 0.7070 - val_loss: 1.0697 - val_acc: 0.6779\n",
            "Epoch 23/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8418 - acc: 0.7099 - val_loss: 0.9869 - val_acc: 0.6889\n",
            "Epoch 24/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8312 - acc: 0.7114 - val_loss: 0.9559 - val_acc: 0.6853\n",
            "Epoch 25/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8219 - acc: 0.7147 - val_loss: 1.0547 - val_acc: 0.6880\n",
            "Epoch 26/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8179 - acc: 0.7177 - val_loss: 0.9026 - val_acc: 0.7048\n",
            "Epoch 27/150\n",
            "1000/1000 [==============================] - 36s 36ms/step - loss: 0.8172 - acc: 0.7191 - val_loss: 0.7884 - val_acc: 0.7393\n",
            "Epoch 28/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8104 - acc: 0.7208 - val_loss: 0.8461 - val_acc: 0.7225\n",
            "Epoch 29/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8104 - acc: 0.7178 - val_loss: 1.1209 - val_acc: 0.6668\n",
            "Epoch 30/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8054 - acc: 0.7199 - val_loss: 0.8796 - val_acc: 0.7140\n",
            "Epoch 31/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8068 - acc: 0.7220 - val_loss: 0.8961 - val_acc: 0.7158\n",
            "Epoch 32/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7984 - acc: 0.7262 - val_loss: 0.9977 - val_acc: 0.6674\n",
            "Epoch 33/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7949 - acc: 0.7237 - val_loss: 0.9091 - val_acc: 0.7041\n",
            "Epoch 34/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7864 - acc: 0.7284 - val_loss: 0.8487 - val_acc: 0.7154\n",
            "Epoch 35/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7813 - acc: 0.7307 - val_loss: 0.8580 - val_acc: 0.7278\n",
            "Epoch 36/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7770 - acc: 0.7296 - val_loss: 0.6804 - val_acc: 0.7716\n",
            "Epoch 37/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7795 - acc: 0.7316 - val_loss: 1.0658 - val_acc: 0.6854\n",
            "Epoch 38/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7786 - acc: 0.7332 - val_loss: 0.8386 - val_acc: 0.7299\n",
            "Epoch 39/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7730 - acc: 0.7315 - val_loss: 1.2945 - val_acc: 0.6053\n",
            "Epoch 40/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7659 - acc: 0.7363 - val_loss: 0.8963 - val_acc: 0.6922\n",
            "Epoch 41/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7723 - acc: 0.7331 - val_loss: 0.9063 - val_acc: 0.7304\n",
            "Epoch 42/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7690 - acc: 0.7342 - val_loss: 0.7438 - val_acc: 0.7565\n",
            "Epoch 43/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7664 - acc: 0.7329 - val_loss: 1.2383 - val_acc: 0.6622\n",
            "Epoch 44/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7668 - acc: 0.7359 - val_loss: 0.7783 - val_acc: 0.7464\n",
            "Epoch 45/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7538 - acc: 0.7385 - val_loss: 1.3705 - val_acc: 0.6544\n",
            "Epoch 46/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7557 - acc: 0.7401 - val_loss: 0.8783 - val_acc: 0.7293\n",
            "Epoch 47/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7458 - acc: 0.7428 - val_loss: 0.8034 - val_acc: 0.7249\n",
            "Epoch 48/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7541 - acc: 0.7409 - val_loss: 0.8449 - val_acc: 0.7348\n",
            "Epoch 49/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7434 - acc: 0.7422 - val_loss: 0.7680 - val_acc: 0.7434\n",
            "Epoch 50/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7467 - acc: 0.7431 - val_loss: 0.9461 - val_acc: 0.7010\n",
            "Epoch 51/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7416 - acc: 0.7434 - val_loss: 0.8288 - val_acc: 0.7214\n",
            "Epoch 52/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7392 - acc: 0.7418 - val_loss: 0.8111 - val_acc: 0.7343\n",
            "Epoch 53/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7439 - acc: 0.7425 - val_loss: 0.8905 - val_acc: 0.7148\n",
            "Epoch 54/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7365 - acc: 0.7469 - val_loss: 0.7350 - val_acc: 0.7701\n",
            "Epoch 55/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7330 - acc: 0.7473 - val_loss: 1.0014 - val_acc: 0.7048\n",
            "Epoch 56/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7329 - acc: 0.7487 - val_loss: 1.2969 - val_acc: 0.6751\n",
            "Epoch 57/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7316 - acc: 0.7504 - val_loss: 0.7412 - val_acc: 0.7623\n",
            "Epoch 58/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7297 - acc: 0.7477 - val_loss: 0.7698 - val_acc: 0.7594\n",
            "Epoch 59/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7304 - acc: 0.7484 - val_loss: 0.8635 - val_acc: 0.7363\n",
            "Epoch 60/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7246 - acc: 0.7515 - val_loss: 0.8163 - val_acc: 0.7564\n",
            "Epoch 61/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7196 - acc: 0.7514 - val_loss: 0.8938 - val_acc: 0.7367\n",
            "Epoch 62/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7223 - acc: 0.7532 - val_loss: 0.8325 - val_acc: 0.7616\n",
            "Epoch 63/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7192 - acc: 0.7508 - val_loss: 1.0009 - val_acc: 0.6876\n",
            "Epoch 64/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7177 - acc: 0.7524 - val_loss: 0.6450 - val_acc: 0.7828\n",
            "Epoch 65/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7170 - acc: 0.7545 - val_loss: 0.8553 - val_acc: 0.7256\n",
            "Epoch 66/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7246 - acc: 0.7510 - val_loss: 0.8161 - val_acc: 0.7473\n",
            "Epoch 67/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7066 - acc: 0.7543 - val_loss: 0.7394 - val_acc: 0.7599\n",
            "Epoch 68/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7118 - acc: 0.7571 - val_loss: 0.7887 - val_acc: 0.7468\n",
            "Epoch 69/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7113 - acc: 0.7558 - val_loss: 0.7773 - val_acc: 0.7469\n",
            "Epoch 70/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7097 - acc: 0.7562 - val_loss: 0.9089 - val_acc: 0.7287\n",
            "Epoch 71/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7087 - acc: 0.7584 - val_loss: 0.8094 - val_acc: 0.7447\n",
            "Epoch 72/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7137 - acc: 0.7574 - val_loss: 0.8997 - val_acc: 0.7450\n",
            "Epoch 73/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7034 - acc: 0.7578 - val_loss: 0.7110 - val_acc: 0.7682\n",
            "Epoch 74/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7042 - acc: 0.7580 - val_loss: 0.7658 - val_acc: 0.7523\n",
            "Epoch 75/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7050 - acc: 0.7588 - val_loss: 0.8603 - val_acc: 0.7352\n",
            "Epoch 76/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7017 - acc: 0.7587 - val_loss: 0.7804 - val_acc: 0.7535\n",
            "Epoch 77/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7044 - acc: 0.7603 - val_loss: 1.0086 - val_acc: 0.7194\n",
            "Epoch 78/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6993 - acc: 0.7630 - val_loss: 0.7748 - val_acc: 0.7406\n",
            "Epoch 79/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7080 - acc: 0.7582 - val_loss: 1.6135 - val_acc: 0.6236\n",
            "Epoch 80/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6941 - acc: 0.7620 - val_loss: 0.8835 - val_acc: 0.7460\n",
            "Epoch 81/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7050 - acc: 0.7577 - val_loss: 1.3045 - val_acc: 0.7037\n",
            "Epoch 82/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6947 - acc: 0.7617 - val_loss: 0.7149 - val_acc: 0.7540\n",
            "Epoch 83/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6884 - acc: 0.7621 - val_loss: 0.8599 - val_acc: 0.7626\n",
            "Epoch 84/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6886 - acc: 0.7636 - val_loss: 0.8450 - val_acc: 0.7436\n",
            "Epoch 85/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6968 - acc: 0.7620 - val_loss: 0.7156 - val_acc: 0.7668\n",
            "Epoch 86/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6952 - acc: 0.7602 - val_loss: 1.1763 - val_acc: 0.6778\n",
            "Epoch 87/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6858 - acc: 0.7627 - val_loss: 1.1435 - val_acc: 0.7138\n",
            "Epoch 88/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6909 - acc: 0.7626 - val_loss: 1.1655 - val_acc: 0.6941\n",
            "Epoch 89/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6839 - acc: 0.7651 - val_loss: 0.9327 - val_acc: 0.7469\n",
            "Epoch 90/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6870 - acc: 0.7635 - val_loss: 0.9299 - val_acc: 0.7246\n",
            "Epoch 91/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6826 - acc: 0.7660 - val_loss: 0.8273 - val_acc: 0.7466\n",
            "Epoch 92/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6828 - acc: 0.7663 - val_loss: 0.6899 - val_acc: 0.7819\n",
            "Epoch 93/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6813 - acc: 0.7677 - val_loss: 0.7036 - val_acc: 0.7854\n",
            "Epoch 94/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6819 - acc: 0.7658 - val_loss: 0.6773 - val_acc: 0.7838\n",
            "Epoch 95/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6848 - acc: 0.7667 - val_loss: 0.8086 - val_acc: 0.7302\n",
            "Epoch 96/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6815 - acc: 0.7631 - val_loss: 0.8065 - val_acc: 0.7536\n",
            "Epoch 97/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6779 - acc: 0.7706 - val_loss: 0.8495 - val_acc: 0.7363\n",
            "Epoch 98/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6732 - acc: 0.7693 - val_loss: 0.7528 - val_acc: 0.7704\n",
            "Epoch 99/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6756 - acc: 0.7685 - val_loss: 0.9652 - val_acc: 0.7480\n",
            "Epoch 100/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6825 - acc: 0.7653 - val_loss: 0.7591 - val_acc: 0.7702\n",
            "Epoch 101/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6734 - acc: 0.7697 - val_loss: 0.6473 - val_acc: 0.7975\n",
            "Epoch 102/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6738 - acc: 0.7688 - val_loss: 0.7032 - val_acc: 0.7719\n",
            "Epoch 103/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6678 - acc: 0.7686 - val_loss: 0.9652 - val_acc: 0.7350\n",
            "Epoch 104/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6659 - acc: 0.7699 - val_loss: 1.1305 - val_acc: 0.7195\n",
            "Epoch 105/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6637 - acc: 0.7728 - val_loss: 0.7296 - val_acc: 0.7600\n",
            "Epoch 106/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6743 - acc: 0.7705 - val_loss: 0.7666 - val_acc: 0.7518\n",
            "Epoch 107/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6667 - acc: 0.7685 - val_loss: 0.8754 - val_acc: 0.7548\n",
            "Epoch 108/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6612 - acc: 0.7731 - val_loss: 0.9168 - val_acc: 0.7448\n",
            "Epoch 109/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6665 - acc: 0.7692 - val_loss: 0.6695 - val_acc: 0.7800\n",
            "Epoch 110/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6706 - acc: 0.7695 - val_loss: 0.7223 - val_acc: 0.7770\n",
            "Epoch 111/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6691 - acc: 0.7704 - val_loss: 0.7592 - val_acc: 0.7745\n",
            "Epoch 112/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6680 - acc: 0.7689 - val_loss: 0.7449 - val_acc: 0.7536\n",
            "Epoch 113/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6596 - acc: 0.7753 - val_loss: 0.7438 - val_acc: 0.7596\n",
            "Epoch 114/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6669 - acc: 0.7722 - val_loss: 0.9477 - val_acc: 0.7503\n",
            "Epoch 115/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6589 - acc: 0.7721 - val_loss: 0.8235 - val_acc: 0.7401\n",
            "Epoch 116/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6549 - acc: 0.7762 - val_loss: 0.7333 - val_acc: 0.7567\n",
            "Epoch 117/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6608 - acc: 0.7729 - val_loss: 0.6988 - val_acc: 0.7814\n",
            "Epoch 118/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6583 - acc: 0.7738 - val_loss: 0.7014 - val_acc: 0.7719\n",
            "Epoch 119/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6567 - acc: 0.7759 - val_loss: 0.7677 - val_acc: 0.7476\n",
            "Epoch 120/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6582 - acc: 0.7732 - val_loss: 0.6994 - val_acc: 0.7820\n",
            "Epoch 121/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6586 - acc: 0.7736 - val_loss: 0.6459 - val_acc: 0.7909\n",
            "Epoch 122/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6508 - acc: 0.7751 - val_loss: 0.9530 - val_acc: 0.7088\n",
            "Epoch 123/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6592 - acc: 0.7738 - val_loss: 0.7994 - val_acc: 0.7554\n",
            "Epoch 124/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6571 - acc: 0.7732 - val_loss: 0.7958 - val_acc: 0.7583\n",
            "Epoch 125/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6487 - acc: 0.7773 - val_loss: 0.7261 - val_acc: 0.7811\n",
            "Epoch 126/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6517 - acc: 0.7778 - val_loss: 0.8815 - val_acc: 0.7010\n",
            "Epoch 127/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6564 - acc: 0.7753 - val_loss: 0.7075 - val_acc: 0.7697\n",
            "Epoch 128/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6579 - acc: 0.7757 - val_loss: 0.8943 - val_acc: 0.7196\n",
            "Epoch 129/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6516 - acc: 0.7760 - val_loss: 1.1567 - val_acc: 0.7409\n",
            "Epoch 130/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6510 - acc: 0.7783 - val_loss: 0.9322 - val_acc: 0.7497\n",
            "Epoch 131/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6505 - acc: 0.7754 - val_loss: 0.8017 - val_acc: 0.7522\n",
            "Epoch 132/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6484 - acc: 0.7763 - val_loss: 0.9720 - val_acc: 0.7069\n",
            "Epoch 133/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6431 - acc: 0.7792 - val_loss: 1.0560 - val_acc: 0.7589\n",
            "Epoch 134/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6446 - acc: 0.7766 - val_loss: 0.8288 - val_acc: 0.7451\n",
            "Epoch 135/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6345 - acc: 0.7808 - val_loss: 0.9101 - val_acc: 0.7207\n",
            "Epoch 136/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6475 - acc: 0.7783 - val_loss: 0.6892 - val_acc: 0.7863\n",
            "Epoch 137/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6459 - acc: 0.7792 - val_loss: 0.7772 - val_acc: 0.7791\n",
            "Epoch 138/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6463 - acc: 0.7787 - val_loss: 0.7465 - val_acc: 0.7630\n",
            "Epoch 139/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6446 - acc: 0.7789 - val_loss: 0.8365 - val_acc: 0.7570\n",
            "Epoch 140/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6465 - acc: 0.7771 - val_loss: 0.9853 - val_acc: 0.7548\n",
            "Epoch 141/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6445 - acc: 0.7791 - val_loss: 1.0550 - val_acc: 0.7587\n",
            "Epoch 142/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6370 - acc: 0.7797 - val_loss: 0.7548 - val_acc: 0.7586\n",
            "Epoch 143/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6473 - acc: 0.7809 - val_loss: 0.8139 - val_acc: 0.7540\n",
            "Epoch 144/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6440 - acc: 0.7788 - val_loss: 0.8045 - val_acc: 0.7690\n",
            "Epoch 145/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6429 - acc: 0.7800 - val_loss: 0.8856 - val_acc: 0.7220\n",
            "Epoch 146/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6388 - acc: 0.7821 - val_loss: 0.6685 - val_acc: 0.7850\n",
            "Epoch 147/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6412 - acc: 0.7789 - val_loss: 0.7627 - val_acc: 0.7653\n",
            "Epoch 148/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6378 - acc: 0.7807 - val_loss: 0.7645 - val_acc: 0.7446\n",
            "Epoch 149/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6400 - acc: 0.7806 - val_loss: 0.9536 - val_acc: 0.7592\n",
            "Epoch 150/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6275 - acc: 0.7861 - val_loss: 1.0783 - val_acc: 0.6692\n"
          ]
        }
      ],
      "source": [
        "# Fits the model on batches with real-time data augmentation\n",
        "\n",
        "history_augmt = model.fit(train_datagen.flow(x_tr, y_tr, batch_size=40), steps_per_epoch=x_tr.shape[0] // 40, epochs=150, validation_data=(x_val, y_val), validation_batch_size=x_val.shape[0] // 40)\n",
        "model.save('data_augmt_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFyrg0MlfRe"
      },
      "source": [
        "## Plot the training and validation loss curve versus epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6iGRFYJxf-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "71307120-2874-4bee-9616-18053e35bb92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_augmt.history['acc']\n",
        "val_acc = history_augmt.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FODq4jJkmI3s"
      },
      "source": [
        "## Train (again) and evaluate the model\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbskoYCFmWMj"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ss--sTammX0-"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = 0.0001) , metrics=['acc'])\n",
        "\n",
        "# using data augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YiHQcf8AnW32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1451ae6f-0ee8-4770-e27f-3397be5dc49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.6466 - acc: 0.4034\n",
            "Epoch 2/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.4427 - acc: 0.4816\n",
            "Epoch 3/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.3547 - acc: 0.5174\n",
            "Epoch 4/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2874 - acc: 0.5434\n",
            "Epoch 5/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2400 - acc: 0.5596\n",
            "Epoch 6/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2068 - acc: 0.5718\n",
            "Epoch 7/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1824 - acc: 0.5831\n",
            "Epoch 8/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1539 - acc: 0.5899\n",
            "Epoch 9/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1277 - acc: 0.6019\n",
            "Epoch 10/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1102 - acc: 0.6067\n",
            "Epoch 11/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0965 - acc: 0.6154\n",
            "Epoch 12/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0785 - acc: 0.6222\n",
            "Epoch 13/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0686 - acc: 0.6249\n",
            "Epoch 14/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0502 - acc: 0.6309\n",
            "Epoch 15/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0401 - acc: 0.6355\n",
            "Epoch 16/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0250 - acc: 0.6421\n",
            "Epoch 17/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0113 - acc: 0.6444\n",
            "Epoch 18/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0046 - acc: 0.6482\n",
            "Epoch 19/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9917 - acc: 0.6523\n",
            "Epoch 20/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9800 - acc: 0.6562\n",
            "Epoch 21/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9669 - acc: 0.6589\n",
            "Epoch 22/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9572 - acc: 0.6652\n",
            "Epoch 23/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.9545 - acc: 0.6644\n",
            "Epoch 24/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9429 - acc: 0.6698\n",
            "Epoch 25/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9321 - acc: 0.6720\n",
            "Epoch 26/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9286 - acc: 0.6749\n",
            "Epoch 27/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9225 - acc: 0.6799\n",
            "Epoch 28/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9145 - acc: 0.6784\n",
            "Epoch 29/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9060 - acc: 0.6870\n",
            "Epoch 30/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.8989 - acc: 0.6855\n",
            "Epoch 31/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8938 - acc: 0.6893\n",
            "Epoch 32/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8897 - acc: 0.6864\n",
            "Epoch 33/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8822 - acc: 0.6900\n",
            "Epoch 34/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8799 - acc: 0.6918\n",
            "Epoch 35/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8751 - acc: 0.6957\n",
            "Epoch 36/150\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.8687 - acc: 0.6965\n",
            "Epoch 37/150\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 0.8640 - acc: 0.6983\n",
            "Epoch 38/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.8563 - acc: 0.7011\n",
            "Epoch 39/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8562 - acc: 0.7014\n",
            "Epoch 40/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8524 - acc: 0.7049\n",
            "Epoch 41/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8491 - acc: 0.7060\n",
            "Epoch 42/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8405 - acc: 0.7081\n",
            "Epoch 43/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8337 - acc: 0.7093\n",
            "Epoch 44/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8357 - acc: 0.7094\n",
            "Epoch 45/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8291 - acc: 0.7124\n",
            "Epoch 46/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8219 - acc: 0.7138\n",
            "Epoch 47/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8218 - acc: 0.7148\n",
            "Epoch 48/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8219 - acc: 0.7160\n",
            "Epoch 49/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8187 - acc: 0.7148\n",
            "Epoch 50/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8121 - acc: 0.7172\n",
            "Epoch 51/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8125 - acc: 0.7183\n",
            "Epoch 52/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8080 - acc: 0.7181\n",
            "Epoch 53/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8007 - acc: 0.7219\n",
            "Epoch 54/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8038 - acc: 0.7192\n",
            "Epoch 55/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7943 - acc: 0.7238\n",
            "Epoch 56/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7973 - acc: 0.7236\n",
            "Epoch 57/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7917 - acc: 0.7206\n",
            "Epoch 58/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7864 - acc: 0.7242\n",
            "Epoch 59/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7879 - acc: 0.7251\n",
            "Epoch 60/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7862 - acc: 0.7262\n",
            "Epoch 61/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7857 - acc: 0.7266\n",
            "Epoch 62/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7811 - acc: 0.7271\n",
            "Epoch 63/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7771 - acc: 0.7293\n",
            "Epoch 64/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7772 - acc: 0.7295\n",
            "Epoch 65/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7736 - acc: 0.7312\n",
            "Epoch 66/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7646 - acc: 0.7342\n",
            "Epoch 67/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7699 - acc: 0.7321\n",
            "Epoch 68/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7604 - acc: 0.7359\n",
            "Epoch 69/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7650 - acc: 0.7339\n",
            "Epoch 70/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7663 - acc: 0.7321\n",
            "Epoch 71/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7583 - acc: 0.7373\n",
            "Epoch 72/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7565 - acc: 0.7354\n",
            "Epoch 73/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7546 - acc: 0.7366\n",
            "Epoch 74/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7555 - acc: 0.7382\n",
            "Epoch 75/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7501 - acc: 0.7392\n",
            "Epoch 76/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7453 - acc: 0.7436\n",
            "Epoch 77/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7431 - acc: 0.7406\n",
            "Epoch 78/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7467 - acc: 0.7408\n",
            "Epoch 79/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7392 - acc: 0.7429\n",
            "Epoch 80/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7410 - acc: 0.7433\n",
            "Epoch 81/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7445 - acc: 0.7420\n",
            "Epoch 82/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7393 - acc: 0.7437\n",
            "Epoch 83/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7336 - acc: 0.7459\n",
            "Epoch 84/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7335 - acc: 0.7443\n",
            "Epoch 85/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7347 - acc: 0.7450\n",
            "Epoch 86/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7335 - acc: 0.7454\n",
            "Epoch 87/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7295 - acc: 0.7453\n",
            "Epoch 88/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7274 - acc: 0.7465\n",
            "Epoch 89/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7272 - acc: 0.7471\n",
            "Epoch 90/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7262 - acc: 0.7469\n",
            "Epoch 91/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7232 - acc: 0.7489\n",
            "Epoch 92/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7221 - acc: 0.7491\n",
            "Epoch 93/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7273 - acc: 0.7473\n",
            "Epoch 94/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7142 - acc: 0.7529\n",
            "Epoch 95/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7126 - acc: 0.7517\n",
            "Epoch 96/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7188 - acc: 0.7510\n",
            "Epoch 97/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7179 - acc: 0.7509\n",
            "Epoch 98/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7133 - acc: 0.7525\n",
            "Epoch 99/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7117 - acc: 0.7533\n",
            "Epoch 100/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7092 - acc: 0.7537\n",
            "Epoch 101/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7116 - acc: 0.7535\n",
            "Epoch 102/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6994 - acc: 0.7563\n",
            "Epoch 103/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7064 - acc: 0.7545\n",
            "Epoch 104/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7108 - acc: 0.7552\n",
            "Epoch 105/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7056 - acc: 0.7533\n",
            "Epoch 106/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7023 - acc: 0.7555\n",
            "Epoch 107/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7017 - acc: 0.7566\n",
            "Epoch 108/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7021 - acc: 0.7562\n",
            "Epoch 109/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7010 - acc: 0.7578\n",
            "Epoch 110/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6984 - acc: 0.7580\n",
            "Epoch 111/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6946 - acc: 0.7580\n",
            "Epoch 112/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6981 - acc: 0.7575\n",
            "Epoch 113/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6976 - acc: 0.7584\n",
            "Epoch 114/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6925 - acc: 0.7612\n",
            "Epoch 115/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6891 - acc: 0.7591\n",
            "Epoch 116/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6920 - acc: 0.7596\n",
            "Epoch 117/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6867 - acc: 0.7601\n",
            "Epoch 118/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6872 - acc: 0.7613\n",
            "Epoch 119/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6899 - acc: 0.7621\n",
            "Epoch 120/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6905 - acc: 0.7606\n",
            "Epoch 121/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6806 - acc: 0.7641\n",
            "Epoch 122/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6863 - acc: 0.7603\n",
            "Epoch 123/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6831 - acc: 0.7619\n",
            "Epoch 124/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6827 - acc: 0.7614\n",
            "Epoch 125/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6753 - acc: 0.7655\n",
            "Epoch 126/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6813 - acc: 0.7648\n",
            "Epoch 127/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6831 - acc: 0.7644\n",
            "Epoch 128/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6776 - acc: 0.7634\n",
            "Epoch 129/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6778 - acc: 0.7637\n",
            "Epoch 130/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6788 - acc: 0.7647\n",
            "Epoch 131/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6728 - acc: 0.7679\n",
            "Epoch 132/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6766 - acc: 0.7628\n",
            "Epoch 133/150\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 0.6719 - acc: 0.7670\n",
            "Epoch 134/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6726 - acc: 0.7664\n",
            "Epoch 135/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6733 - acc: 0.7666\n",
            "Epoch 136/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6756 - acc: 0.7655\n",
            "Epoch 137/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6666 - acc: 0.7691\n",
            "Epoch 138/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6696 - acc: 0.7681\n",
            "Epoch 139/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6706 - acc: 0.7673\n",
            "Epoch 140/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6652 - acc: 0.7698\n",
            "Epoch 141/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6635 - acc: 0.7705\n",
            "Epoch 142/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6684 - acc: 0.7678\n",
            "Epoch 143/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6643 - acc: 0.7687\n",
            "Epoch 144/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6636 - acc: 0.7697\n",
            "Epoch 145/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6652 - acc: 0.7714\n",
            "Epoch 146/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6654 - acc: 0.7678\n",
            "Epoch 147/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6677 - acc: 0.7675\n",
            "Epoch 148/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6584 - acc: 0.7711\n",
            "Epoch 149/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6650 - acc: 0.7711\n",
            "Epoch 150/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6609 - acc: 0.7703\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_final_augmt = model.fit(train_datagen.flow(x_train, y_train_vec, batch_size=40), steps_per_epoch=x_tr.shape[0] // 32, epochs=150)\n",
        "model.save('final_data_augmt_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5g8Ut5jVey"
      },
      "source": [
        "## Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCU9xHzrxfyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df08dd63-8a68-4646-856a-48337307f5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.1030 - acc: 0.6632\n",
            "loss = 1.1030269861221313\n",
            "accuracy = 0.6632000207901001\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('data_augmt_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dQJdtdcUjmJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039279fe-d934-41c6-e862-9825fa51f923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6504 - acc: 0.7792\n",
            "loss = 0.6504468321800232\n",
            "accuracy = 0.77920001745224\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('final_data_augmt_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6maaDzvkJep"
      },
      "source": [
        "## Model 2 (BN + Dropout):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK5g8oPUSUsh",
        "outputId": "2ac309d0-6b04-4481-a11c-a6f1351a0d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 627,786\n",
            "Trainable params: 627,082\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trBXxU8bUVtc"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1E-4), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yquECgX7VPs5",
        "outputId": "1ef6b059-f164-4892-c105-7e831c4e9df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 24s 13ms/step - loss: 1.7040 - acc: 0.3880 - val_loss: 1.3660 - val_acc: 0.5193\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.3742 - acc: 0.5095 - val_loss: 1.1906 - val_acc: 0.5855\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.2522 - acc: 0.5553 - val_loss: 1.1191 - val_acc: 0.6135\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.1671 - acc: 0.5859 - val_loss: 1.0684 - val_acc: 0.6276\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 1.1046 - acc: 0.6089 - val_loss: 1.0469 - val_acc: 0.6293\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.0523 - acc: 0.6282 - val_loss: 1.0066 - val_acc: 0.6422\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.0140 - acc: 0.6443 - val_loss: 0.9483 - val_acc: 0.6655\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9842 - acc: 0.6540 - val_loss: 0.9235 - val_acc: 0.6756\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9523 - acc: 0.6636 - val_loss: 0.8803 - val_acc: 0.6930\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9250 - acc: 0.6772 - val_loss: 0.9160 - val_acc: 0.6748\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9027 - acc: 0.6822 - val_loss: 0.9591 - val_acc: 0.6602\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8807 - acc: 0.6909 - val_loss: 0.8610 - val_acc: 0.6982\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8621 - acc: 0.6963 - val_loss: 0.8488 - val_acc: 0.6997\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8435 - acc: 0.7062 - val_loss: 0.8221 - val_acc: 0.7056\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8195 - acc: 0.7138 - val_loss: 0.8080 - val_acc: 0.7180\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8013 - acc: 0.7219 - val_loss: 0.8280 - val_acc: 0.7148\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.7827 - acc: 0.7292 - val_loss: 0.8167 - val_acc: 0.7087\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7777 - acc: 0.7288 - val_loss: 0.7724 - val_acc: 0.7322\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7606 - acc: 0.7362 - val_loss: 0.7725 - val_acc: 0.7342\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7417 - acc: 0.7424 - val_loss: 0.7946 - val_acc: 0.7286\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7319 - acc: 0.7450 - val_loss: 0.7899 - val_acc: 0.7227\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7154 - acc: 0.7516 - val_loss: 0.7483 - val_acc: 0.7417\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7046 - acc: 0.7533 - val_loss: 0.7359 - val_acc: 0.7460\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6928 - acc: 0.7580 - val_loss: 0.8648 - val_acc: 0.7117\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6738 - acc: 0.7659 - val_loss: 0.7693 - val_acc: 0.7328\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6667 - acc: 0.7703 - val_loss: 0.7154 - val_acc: 0.7566\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6599 - acc: 0.7716 - val_loss: 0.7636 - val_acc: 0.7341\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6474 - acc: 0.7761 - val_loss: 0.7245 - val_acc: 0.7493\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6364 - acc: 0.7790 - val_loss: 0.7019 - val_acc: 0.7572\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6241 - acc: 0.7822 - val_loss: 0.7398 - val_acc: 0.7423\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6146 - acc: 0.7866 - val_loss: 0.7429 - val_acc: 0.7441\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6092 - acc: 0.7884 - val_loss: 0.7060 - val_acc: 0.7582\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5970 - acc: 0.7933 - val_loss: 0.7214 - val_acc: 0.7545\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5849 - acc: 0.7966 - val_loss: 0.7178 - val_acc: 0.7543\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5795 - acc: 0.7994 - val_loss: 0.6954 - val_acc: 0.7585\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5717 - acc: 0.8003 - val_loss: 0.7419 - val_acc: 0.7456\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5615 - acc: 0.8029 - val_loss: 0.6835 - val_acc: 0.7693\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5518 - acc: 0.8084 - val_loss: 0.6824 - val_acc: 0.7659\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5446 - acc: 0.8111 - val_loss: 0.6702 - val_acc: 0.7682\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5382 - acc: 0.8149 - val_loss: 0.6832 - val_acc: 0.7683\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5278 - acc: 0.8174 - val_loss: 0.7126 - val_acc: 0.7589\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5197 - acc: 0.8186 - val_loss: 0.7524 - val_acc: 0.7481\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5114 - acc: 0.8225 - val_loss: 0.7067 - val_acc: 0.7574\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5062 - acc: 0.8236 - val_loss: 0.7460 - val_acc: 0.7465\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5020 - acc: 0.8258 - val_loss: 0.6656 - val_acc: 0.7752\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4925 - acc: 0.8275 - val_loss: 0.6965 - val_acc: 0.7640\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4869 - acc: 0.8332 - val_loss: 0.7278 - val_acc: 0.7536\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4773 - acc: 0.8352 - val_loss: 0.6779 - val_acc: 0.7696\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4746 - acc: 0.8363 - val_loss: 0.7929 - val_acc: 0.7369\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4649 - acc: 0.8406 - val_loss: 0.7003 - val_acc: 0.7598\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4617 - acc: 0.8422 - val_loss: 0.7338 - val_acc: 0.7508\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4546 - acc: 0.8450 - val_loss: 0.7171 - val_acc: 0.7514\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4432 - acc: 0.8462 - val_loss: 0.6712 - val_acc: 0.7703\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4422 - acc: 0.8472 - val_loss: 0.6414 - val_acc: 0.7814\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4398 - acc: 0.8485 - val_loss: 0.6455 - val_acc: 0.7779\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4284 - acc: 0.8510 - val_loss: 0.6621 - val_acc: 0.7787\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4242 - acc: 0.8547 - val_loss: 0.8166 - val_acc: 0.7352\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4181 - acc: 0.8570 - val_loss: 0.6570 - val_acc: 0.7796\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 16s 16ms/step - loss: 0.4140 - acc: 0.8557 - val_loss: 0.6628 - val_acc: 0.7750\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4062 - acc: 0.8604 - val_loss: 0.7828 - val_acc: 0.7464\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4058 - acc: 0.8601 - val_loss: 0.7846 - val_acc: 0.7423\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3975 - acc: 0.8644 - val_loss: 0.7174 - val_acc: 0.7660\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3905 - acc: 0.8640 - val_loss: 0.6950 - val_acc: 0.7666\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3935 - acc: 0.8661 - val_loss: 0.7038 - val_acc: 0.7708\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3878 - acc: 0.8654 - val_loss: 0.6466 - val_acc: 0.7815\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3813 - acc: 0.8682 - val_loss: 0.6414 - val_acc: 0.7849\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3835 - acc: 0.8683 - val_loss: 0.7575 - val_acc: 0.7544\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3722 - acc: 0.8713 - val_loss: 0.6492 - val_acc: 0.7849\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3689 - acc: 0.8728 - val_loss: 0.7663 - val_acc: 0.7502\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3640 - acc: 0.8756 - val_loss: 0.6564 - val_acc: 0.7892\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3609 - acc: 0.8763 - val_loss: 0.6462 - val_acc: 0.7866\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3580 - acc: 0.8770 - val_loss: 0.6548 - val_acc: 0.7830\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3515 - acc: 0.8797 - val_loss: 0.7387 - val_acc: 0.7681\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3486 - acc: 0.8799 - val_loss: 0.6677 - val_acc: 0.7770\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3437 - acc: 0.8801 - val_loss: 0.6879 - val_acc: 0.7795\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3342 - acc: 0.8855 - val_loss: 0.7173 - val_acc: 0.7605\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3361 - acc: 0.8865 - val_loss: 0.6395 - val_acc: 0.7866\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3317 - acc: 0.8866 - val_loss: 0.6860 - val_acc: 0.7805\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3289 - acc: 0.8874 - val_loss: 0.7070 - val_acc: 0.7640\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3267 - acc: 0.8874 - val_loss: 0.6917 - val_acc: 0.7788\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3230 - acc: 0.8886 - val_loss: 0.7223 - val_acc: 0.7748\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3233 - acc: 0.8882 - val_loss: 0.6497 - val_acc: 0.7900\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3174 - acc: 0.8918 - val_loss: 0.7188 - val_acc: 0.7695\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3116 - acc: 0.8921 - val_loss: 0.6681 - val_acc: 0.7771\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3132 - acc: 0.8927 - val_loss: 0.6722 - val_acc: 0.7865\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3082 - acc: 0.8955 - val_loss: 0.7259 - val_acc: 0.7734\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3065 - acc: 0.8947 - val_loss: 0.7262 - val_acc: 0.7753\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3085 - acc: 0.8938 - val_loss: 0.6651 - val_acc: 0.7862\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3009 - acc: 0.8980 - val_loss: 0.6699 - val_acc: 0.7821\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2962 - acc: 0.8994 - val_loss: 0.6654 - val_acc: 0.7804\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2915 - acc: 0.8985 - val_loss: 0.6982 - val_acc: 0.7791\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2904 - acc: 0.9004 - val_loss: 0.6765 - val_acc: 0.7806\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2921 - acc: 0.8965 - val_loss: 0.6644 - val_acc: 0.7817\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2833 - acc: 0.9026 - val_loss: 0.6792 - val_acc: 0.7843\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2838 - acc: 0.9017 - val_loss: 0.6548 - val_acc: 0.7873\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2793 - acc: 0.9047 - val_loss: 0.7371 - val_acc: 0.7608\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2793 - acc: 0.9039 - val_loss: 0.6673 - val_acc: 0.7877\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2770 - acc: 0.9036 - val_loss: 0.8245 - val_acc: 0.7562\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2728 - acc: 0.9060 - val_loss: 0.7120 - val_acc: 0.7789\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2736 - acc: 0.9067 - val_loss: 0.6660 - val_acc: 0.7850\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_bn_dropout = model.fit(x_tr, y_tr, batch_size=40, epochs=100, validation_data=(x_val, y_val))\n",
        "model.save('bn_dropout_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvB3zvKHlq0b"
      },
      "source": [
        "## Plot the training and validation loss curve versus epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGAjrUIOlvvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a0537781-633c-4f6a-9c53-bac6bc806762"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_bn_dropout.history['acc']\n",
        "val_acc = history_bn_dropout.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaL358TXmgcA"
      },
      "source": [
        "## Train (again) and evaluate the model\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMWTGOEWmk6_"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfPOOPNpn_wO"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1E-4), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_bn_dropout = model.fit(x_train, y_train_vec, batch_size=40, epochs=100)\n",
        "model.save('final_bn_dropout_model.h5')"
      ],
      "metadata": {
        "id": "m9mxYJEmxEyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6393c1-12c0-4e17-ec9d-5c2555204631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 25s 11ms/step - loss: 1.6303 - acc: 0.4181\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2876 - acc: 0.5449\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1565 - acc: 0.5913\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0758 - acc: 0.6216\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.0230 - acc: 0.6397\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9755 - acc: 0.6581\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9426 - acc: 0.6685\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9091 - acc: 0.6832\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8833 - acc: 0.6879\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8603 - acc: 0.6982\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8399 - acc: 0.7050\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.8178 - acc: 0.7133\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8020 - acc: 0.7199\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7809 - acc: 0.7271\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7633 - acc: 0.7349\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7480 - acc: 0.7370\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7313 - acc: 0.7453\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7196 - acc: 0.7480\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7084 - acc: 0.7518\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6898 - acc: 0.7598\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6831 - acc: 0.7640\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6692 - acc: 0.7674\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6527 - acc: 0.7732\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6452 - acc: 0.7745\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6336 - acc: 0.7797\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6254 - acc: 0.7822\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6170 - acc: 0.7857\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6048 - acc: 0.7900\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5988 - acc: 0.7943\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5889 - acc: 0.7967\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5793 - acc: 0.7997\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5691 - acc: 0.8041\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5627 - acc: 0.8059\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5530 - acc: 0.8096\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5415 - acc: 0.8137\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5391 - acc: 0.8131\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5339 - acc: 0.8150\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.5191 - acc: 0.8209\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5129 - acc: 0.8225\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5065 - acc: 0.8242\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5001 - acc: 0.8296\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4940 - acc: 0.8297\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4888 - acc: 0.8325\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4829 - acc: 0.8311\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4777 - acc: 0.8358\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4648 - acc: 0.8390\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4672 - acc: 0.8364\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4605 - acc: 0.8427\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4491 - acc: 0.8439\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4485 - acc: 0.8450\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4402 - acc: 0.8476\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4367 - acc: 0.8486\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4310 - acc: 0.8516\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4253 - acc: 0.8529\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.4239 - acc: 0.8534\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4112 - acc: 0.8586\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4120 - acc: 0.8574\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4062 - acc: 0.8596\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4005 - acc: 0.8622\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3974 - acc: 0.8636\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3915 - acc: 0.8639\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3917 - acc: 0.8652\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3876 - acc: 0.8667\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3797 - acc: 0.8708\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3770 - acc: 0.8693\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3730 - acc: 0.8720\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3690 - acc: 0.8735\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3669 - acc: 0.8745\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3587 - acc: 0.8750\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3580 - acc: 0.8758\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3529 - acc: 0.8785\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3522 - acc: 0.8796\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3490 - acc: 0.8777\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3459 - acc: 0.8803\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3412 - acc: 0.8820\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3369 - acc: 0.8832\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3322 - acc: 0.8862\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3378 - acc: 0.8815\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3275 - acc: 0.8862\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3235 - acc: 0.8893\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3226 - acc: 0.8878\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3160 - acc: 0.8907\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3157 - acc: 0.8912\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3106 - acc: 0.8941\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3135 - acc: 0.8921\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3129 - acc: 0.8921\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3101 - acc: 0.8931\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3087 - acc: 0.8917\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3013 - acc: 0.8952\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3025 - acc: 0.8968\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3011 - acc: 0.8956\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3005 - acc: 0.8959\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2983 - acc: 0.8971\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2918 - acc: 0.8994\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2933 - acc: 0.8979\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2910 - acc: 0.8997\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2882 - acc: 0.9018\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2862 - acc: 0.9011\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2822 - acc: 0.9021\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2813 - acc: 0.9018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2u-gWd8kQsn"
      },
      "source": [
        "##  Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8k7gqRrkZIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ef1dd9-79c4-431c-d8de-042922a7de56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6877 - acc: 0.7817\n",
            "loss = 0.6877126097679138\n",
            "accuracy = 0.7817000150680542\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('bn_dropout_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEDGxtHTkdgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc7bdd3-19ea-44a8-b9b1-d8b9d3df541f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6445 - acc: 0.7943\n",
            "loss = 0.6444640159606934\n",
            "accuracy = 0.7943000197410583\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('final_bn_dropout_model.h5')\n",
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparision and Analysis:"
      ],
      "metadata": {
        "id": "mtbj2BDevXsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***Original CNN model:*** has a accuracy of **69.04 %** and after training the network on the entire training set has accuracy of **70.35 %**.\n",
        "- ***Model 1 (BN + Data_Augmentation):*** has a accuracy of **66.32 %** and after training the network on the entire training set has accuracy of **77.92** %. \n",
        "- ***Model 2 (BN + Dropout):*** has a accuracy of **78.17 %** and after training the network on the entire training set has accuracy of **79.43 %**.\n",
        "\n",
        "**~** Clearly ***Model 2*** achieves the best accuracy amongst the above models.\n",
        "\n",
        "**~** Model 1 may achieve more accuracy if the model is trained for more epochs but it is not possible due to limited access to GPU in Google Colab \n",
        "\n"
      ],
      "metadata": {
        "id": "uCsG0mvrveLz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}