{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8l-y1atoOa"
      },
      "source": [
        "# HW1: Logistic Regression.\n",
        "\n",
        "### Name: [Daksh Bhuva]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPOHxTzTtoOc"
      },
      "source": [
        "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
        "\n",
        "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qv8XuUJEtoOc"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpZrOLY6toOd"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocexfjrrtoOd"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "kPesOAxZtoOd",
        "outputId": "f2732705-5e8b-4895-a6ca-440e95a764cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-60739370-0217-4daf-b427-724ccf837cb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60739370-0217-4daf-b427-724ccf837cb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60739370-0217-4daf-b427-724ccf837cb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60739370-0217-4daf-b427-724ccf837cb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "dataset = pd.read_csv('/content/data.csv')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCIji6zHtoOd"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qknohm9WtoOe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1d4a6b71-70e3-44c7-e388-07393911abce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b280a798-3061-4ce2-9b54-823ee8be70f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b280a798-3061-4ce2-9b54-823ee8be70f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b280a798-3061-4ce2-9b54-823ee8be70f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b280a798-3061-4ce2-9b54-823ee8be70f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0         -1        17.99  ...          0.4601                  0.11890\n",
              "1         -1        20.57  ...          0.2750                  0.08902\n",
              "2         -1        19.69  ...          0.3613                  0.08758\n",
              "3         -1        11.42  ...          0.6638                  0.17300\n",
              "4         -1        20.29  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
        "# You need to get rid of the ID number feature.\n",
        "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "\n",
        "dataset.drop(dataset.columns[len(dataset.columns)-1], axis=1, inplace=True)\n",
        "dataset.drop(dataset.columns[0], axis=1, inplace=True)\n",
        "dataset.diagnosis = dataset.diagnosis.map({\"B\": 1, \"M\": -1})\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKAQ0J_StoOe"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y is the Target attribute 'diagnosis' and X is rest of the attributes\n",
        "\n",
        "dvalues = dataset.values\n",
        "X, y = dvalues[:, 1:], dvalues[:, 0:1]"
      ],
      "metadata": {
        "id": "Yf7sIs8AzUvZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FZ75fikytoOe"
      },
      "outputs": [],
      "source": [
        "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH5hX6oStoOf"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvvzeoxtoOf"
      },
      "source": [
        "Use the standardization to trainsform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo7WXTVJtoOf",
        "outputId": "8638133a-6427-4bb2-f48e-2d69bc06ab67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test mean = \n",
            "[ 0.02090745  0.16431943  0.03098111  0.01492717 -0.01921133  0.10117201\n",
            "  0.08443932  0.02966041  0.04062134  0.09494587  0.04653776  0.06428674\n",
            "  0.10007498  0.00538732  0.06601984  0.11839521  0.0676146   0.1268942\n",
            "  0.01037956  0.16193656  0.01337734  0.1134187   0.03390329 -0.00517026\n",
            " -0.02001553  0.07842781  0.11756424  0.01212816 -0.02700714  0.12744666]\n",
            "test std = \n",
            "[0.98484027 1.15221324 0.99783436 0.91061412 1.0932457  1.2099472\n",
            " 1.1161288  1.0269985  1.00338544 1.16387855 0.86929628 1.0858163\n",
            " 0.95737985 0.73797557 1.25342805 1.10484161 0.84117872 1.20913453\n",
            " 1.04276269 1.05856313 0.94112163 1.03290181 0.96071251 0.84629366\n",
            " 1.04566915 1.12995399 1.20449902 1.03206998 0.90827971 1.06240744]\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "import numpy\n",
        "\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
        "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(numpy.mean(x_test, axis=0))\n",
        "\n",
        "print('test std = ')\n",
        "print(numpy.std(x_test, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqPtf0ttoOf"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IVyr570qtoOg"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "\n",
        "def objective(w, x, y, lam):\n",
        "    expTerm = np.exp((-(np.dot(np.multiply(y, x), w))))\n",
        "    logTerm = np.log(1 + expTerm)\n",
        "\n",
        "    return np.mean(logTerm) + (lam/2 * np.sum(w*w))\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FWtKaAKtoOg"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6OW58rdtoOg"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc_MHHojtoOg"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BqdeMvvUtoOg"
      },
      "outputs": [],
      "source": [
        "# Calculate the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "    n, d = x.shape\n",
        "    numerator = np.multiply(y, x)\n",
        "    denominator = 1 + np.exp(((np.dot(np.multiply(y, x), w))))\n",
        "    meanTerm = np.divide(numerator, denominator)\n",
        "\n",
        "    return -np.mean(meanTerm, axis=0).reshape(d, 1) + lam*w \n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z5ciUYzatoOg"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    n, d = x.shape\n",
        "    objvals = numpy.zeros(max_epoch)\n",
        "    \n",
        "    for i in range(max_epoch):\n",
        "        objval = objective(w, x, y, lam)\n",
        "        objvals[i] = objval\n",
        "        print('Objective value at ' + str(i+1) + ' is ' + str(objval))\n",
        "        gradientTerm = gradient(w, x, y, lam)\n",
        "        w = w - learning_rate * gradientTerm\n",
        "    \n",
        "    return w, objvals\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGrFhd-NtoOg"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "W7qg1_w-toOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae29e596-ebc0-4b6b-a7a0-53cc6807f0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at 1 is 0.6931471805599453\n",
            "Objective value at 2 is 0.1736337296064684\n",
            "Objective value at 3 is 0.13863877872989974\n",
            "Objective value at 4 is 0.12288413006561809\n",
            "Objective value at 5 is 0.11576613492616913\n",
            "Objective value at 6 is 0.11099019308816198\n",
            "Objective value at 7 is 0.10717885662856941\n",
            "Objective value at 8 is 0.10396949092070193\n",
            "Objective value at 9 is 0.10119870557399414\n",
            "Objective value at 10 is 0.09876897867939638\n",
            "Objective value at 11 is 0.09661369678769466\n",
            "Objective value at 12 is 0.09468412359097389\n",
            "Objective value at 13 is 0.09294316237442035\n",
            "Objective value at 14 is 0.09136183064631986\n",
            "Objective value at 15 is 0.08991704767563728\n",
            "Objective value at 16 is 0.08859015119945056\n",
            "Objective value at 17 is 0.08736585870901725\n",
            "Objective value at 18 is 0.08623151738983048\n",
            "Objective value at 19 is 0.08517654982524114\n",
            "Objective value at 20 is 0.08419203673857761\n",
            "Objective value at 21 is 0.08327039802143471\n",
            "Objective value at 22 is 0.08240514564272644\n",
            "Objective value at 23 is 0.08159068999114516\n",
            "Objective value at 24 is 0.08082218649967549\n",
            "Objective value at 25 is 0.08009541301570064\n",
            "Objective value at 26 is 0.07940667089937643\n",
            "Objective value at 27 is 0.07875270461947759\n",
            "Objective value at 28 is 0.07813063590222333\n",
            "Objective value at 29 is 0.07753790942718333\n",
            "Objective value at 30 is 0.07697224775749245\n",
            "Objective value at 31 is 0.07643161370905327\n",
            "Objective value at 32 is 0.07591417875356758\n",
            "Objective value at 33 is 0.07541829634714227\n",
            "Objective value at 34 is 0.07494247930408528\n",
            "Objective value at 35 is 0.07448538051179412\n",
            "Objective value at 36 is 0.07404577642004123\n",
            "Objective value at 37 is 0.07362255284580577\n",
            "Objective value at 38 is 0.07321469272001259\n",
            "Objective value at 39 is 0.07282126547028489\n",
            "Objective value at 40 is 0.07244141778799985\n",
            "Objective value at 41 is 0.07207436557151471\n",
            "Objective value at 42 is 0.07171938687266587\n",
            "Objective value at 43 is 0.07137581570228162\n",
            "Objective value at 44 is 0.07104303657383451\n",
            "Objective value at 45 is 0.07072047968354749\n",
            "Objective value at 46 is 0.07040761664107927\n",
            "Objective value at 47 is 0.07010395667800033\n",
            "Objective value at 48 is 0.06980904327214489\n",
            "Objective value at 49 is 0.06952245113499397\n",
            "Objective value at 50 is 0.06924378351684055\n",
            "Objective value at 51 is 0.0689726697908693\n",
            "Objective value at 52 is 0.0687087632826653\n",
            "Objective value at 53 is 0.06845173931621928\n",
            "Objective value at 54 is 0.06820129345136096\n",
            "Objective value at 55 is 0.06795713989084202\n",
            "Objective value at 56 is 0.0677190100380999\n",
            "Objective value at 57 is 0.06748665118913857\n",
            "Objective value at 58 is 0.06725982534402884\n",
            "Objective value at 59 is 0.06703830812530863\n",
            "Objective value at 60 is 0.06682188779210005\n",
            "Objective value at 61 is 0.06661036434008735\n",
            "Objective value at 62 is 0.06640354867865406\n",
            "Objective value at 63 is 0.06620126187747809\n",
            "Objective value at 64 is 0.06600333447575849\n",
            "Objective value at 65 is 0.06580960584800918\n",
            "Objective value at 66 is 0.06561992362102323\n",
            "Objective value at 67 is 0.06543414313719645\n",
            "Objective value at 68 is 0.06525212695991378\n",
            "Objective value at 69 is 0.06507374441715513\n",
            "Objective value at 70 is 0.06489887117987654\n",
            "Objective value at 71 is 0.06472738887207616\n",
            "Objective value at 72 is 0.06455918470976725\n",
            "Objective value at 73 is 0.06439415116635731\n",
            "Objective value at 74 is 0.06423218566217935\n",
            "Objective value at 75 is 0.06407319027614\n",
            "Objective value at 76 is 0.06391707147764517\n",
            "Objective value at 77 is 0.06376373987713738\n",
            "Objective value at 78 is 0.06361310999373587\n",
            "Objective value at 79 is 0.06346510003860935\n",
            "Objective value at 80 is 0.06331963171283633\n",
            "Objective value at 81 is 0.06317663001862125\n",
            "Objective value at 82 is 0.0630360230828342\n",
            "Objective value at 83 is 0.06289774199193376\n",
            "Objective value at 84 is 0.06276172063741477\n",
            "Objective value at 85 is 0.06262789557099521\n",
            "Objective value at 86 is 0.06249620586882527\n",
            "Objective value at 87 is 0.062366593004060285\n",
            "Objective value at 88 is 0.062239000727194876\n",
            "Objective value at 89 is 0.0621133749536052\n",
            "Objective value at 90 is 0.06198966365779098\n",
            "Objective value at 91 is 0.06186781677385\n",
            "Objective value at 92 is 0.06174778610175523\n",
            "Objective value at 93 is 0.06162952521903818\n",
            "Objective value at 94 is 0.061512989397513596\n",
            "Objective value at 95 is 0.06139813552470812\n",
            "Objective value at 96 is 0.0612849220296821\n",
            "Objective value at 97 is 0.061173308812956365\n",
            "Objective value at 98 is 0.0610632571802783\n",
            "Objective value at 99 is 0.06095472977998051\n",
            "Objective value at 100 is 0.06084769054370377\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0\n",
        "learning_rate = 1\n",
        "w = np.zeros((d, 1))\n",
        "\n",
        "w_gd, objvals_gd = gradient_descent(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-Gp9oeB_toOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194bfaf3-1f35-45db-ebf2-8be0a0dfd18d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at 1 is 0.6931471805599453\n",
            "Objective value at 2 is 0.17373075003835395\n",
            "Objective value at 3 is 0.13874590240831533\n",
            "Objective value at 4 is 0.12300803297716084\n",
            "Objective value at 5 is 0.11590597229060243\n",
            "Objective value at 6 is 0.11114405534689033\n",
            "Objective value at 7 is 0.1073455650071604\n",
            "Objective value at 8 is 0.10414818231634433\n",
            "Objective value at 9 is 0.10138869528550187\n",
            "Objective value at 10 is 0.098969698632984\n",
            "Objective value at 11 is 0.09682466165724318\n",
            "Objective value at 12 is 0.09490491016451823\n",
            "Objective value at 13 is 0.09317339603865071\n",
            "Objective value at 14 is 0.09160117601453352\n",
            "Objective value at 15 is 0.09016520178393278\n",
            "Objective value at 16 is 0.08884683838130639\n",
            "Objective value at 17 is 0.08763082661748482\n",
            "Objective value at 18 is 0.08650453383526606\n",
            "Objective value at 19 is 0.08545740021181764\n",
            "Objective value at 20 is 0.08448052195268194\n",
            "Objective value at 21 is 0.08356633266906562\n",
            "Objective value at 22 is 0.08270835656172436\n",
            "Objective value at 23 is 0.0819010149834316\n",
            "Objective value at 24 is 0.08113947324208864\n",
            "Objective value at 25 is 0.0804195181175506\n",
            "Objective value at 26 is 0.0797374590817802\n",
            "Objective value at 27 is 0.07909004799668465\n",
            "Objective value at 28 is 0.07847441334903354\n",
            "Objective value at 29 is 0.0778880060195481\n",
            "Objective value at 30 is 0.07732855427571582\n",
            "Objective value at 31 is 0.07679402619484421\n",
            "Objective value at 32 is 0.07628259811365644\n",
            "Objective value at 33 is 0.0757926279973506\n",
            "Objective value at 34 is 0.07532263284869416\n",
            "Objective value at 35 is 0.07487126945383744\n",
            "Objective value at 36 is 0.07443731789879512\n",
            "Objective value at 37 is 0.07401966739827946\n",
            "Objective value at 38 is 0.07361730406369241\n",
            "Objective value at 39 is 0.07322930030475672\n",
            "Objective value at 40 is 0.07285480561339171\n",
            "Objective value at 41 is 0.0724930385219668\n",
            "Objective value at 42 is 0.072143279563264\n",
            "Objective value at 43 is 0.0718048650880834\n",
            "Objective value at 44 is 0.07147718181978374\n",
            "Objective value at 45 is 0.07115966204421587\n",
            "Objective value at 46 is 0.0708517793492974\n",
            "Objective value at 47 is 0.07055304484154769\n",
            "Objective value at 48 is 0.0702630037777602\n",
            "Objective value at 49 is 0.06998123255904957\n",
            "Objective value at 50 is 0.06970733604209407\n",
            "Objective value at 51 is 0.06944094512876808\n",
            "Objective value at 52 is 0.06918171460073362\n",
            "Objective value at 53 is 0.06892932117010583\n",
            "Objective value at 54 is 0.06868346172116638\n",
            "Objective value at 55 is 0.0684438517213838\n",
            "Objective value at 56 is 0.06821022378280486\n",
            "Objective value at 57 is 0.06798232635728214\n",
            "Objective value at 58 is 0.0677599225510672\n",
            "Objective value at 59 is 0.06754278904607275\n",
            "Objective value at 60 is 0.067330715116641\n",
            "Objective value at 61 is 0.06712350173198207\n",
            "Objective value at 62 is 0.066920960735596\n",
            "Objective value at 63 is 0.0667229140939937\n",
            "Objective value at 64 is 0.06652919320790278\n",
            "Objective value at 65 is 0.06633963827990645\n",
            "Objective value at 66 is 0.06615409773313002\n",
            "Objective value at 67 is 0.06597242767617341\n",
            "Objective value at 68 is 0.0657944914100026\n",
            "Objective value at 69 is 0.06562015897296444\n",
            "Objective value at 70 is 0.06544930672048832\n",
            "Objective value at 71 is 0.06528181693639107\n",
            "Objective value at 72 is 0.06511757747301318\n",
            "Objective value at 73 is 0.06495648141769135\n",
            "Objective value at 74 is 0.0647984267833182\n",
            "Objective value at 75 is 0.06464331622095892\n",
            "Objective value at 76 is 0.06449105675268933\n",
            "Objective value at 77 is 0.0643415595229942\n",
            "Objective value at 78 is 0.06419473956721959\n",
            "Objective value at 79 is 0.06405051559571306\n",
            "Objective value at 80 is 0.06390880979240993\n",
            "Objective value at 81 is 0.0637695476267355\n",
            "Objective value at 82 is 0.06363265767779511\n",
            "Objective value at 83 is 0.06349807146991286\n",
            "Objective value at 84 is 0.06336572331866333\n",
            "Objective value at 85 is 0.06323555018661277\n",
            "Objective value at 86 is 0.06310749154805434\n",
            "Objective value at 87 is 0.0629814892620811\n",
            "Objective value at 88 is 0.06285748745339563\n",
            "Objective value at 89 is 0.06273543240030455\n",
            "Objective value at 90 is 0.06261527242939131\n",
            "Objective value at 91 is 0.062496957816400855\n",
            "Objective value at 92 is 0.06238044069290791\n",
            "Objective value at 93 is 0.06226567495837325\n",
            "Objective value at 94 is 0.06215261619722441\n",
            "Objective value at 95 is 0.062041221600624384\n",
            "Objective value at 96 is 0.06193144989261794\n",
            "Objective value at 97 is 0.0618232612603692\n",
            "Objective value at 98 is 0.06171661728822443\n",
            "Objective value at 99 is 0.061611480895354946\n",
            "Objective value at 100 is 0.06150781627675221\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0.0001\n",
        "learning_rate = 1\n",
        "w = np.zeros((d, 1))\n",
        "\n",
        "w_gd_regularized, objvals_gd_regularized = gradient_descent(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78sVFX-ItoOh"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iiI0y-x8toOh"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "    d = xi.shape[0]\n",
        "    expTerm = float(np.dot(yi * xi, w))\n",
        "    logTerm = np.log(1 + np.exp(-expTerm)) \n",
        "    obj = logTerm + lam/2 * np.sum(w*w) \n",
        "    \n",
        "    gTerm = -(yi * xi).T / (1 + np.exp(expTerm)) \n",
        "    g = gTerm + lam * w\n",
        "    \n",
        "    return obj, g\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2muy2gPtoOh"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples.\n",
        "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ovPRWupOtoOh"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     \n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    n, d = x.shape\n",
        "    objvals = np.zeros(max_epoch) \n",
        "    \n",
        "    for i in range(max_epoch):\n",
        "        \n",
        "        rand_indices = numpy.random.permutation(n)\n",
        "        x_rand = x[rand_indices, :]\n",
        "        y_rand = y[rand_indices, :]\n",
        "        objval = 0 \n",
        "        for j in range(n):\n",
        "            xi = x_rand[j, :].reshape((1,d))\n",
        "            yi = float(y_rand[j, :]) \n",
        "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "            objval = objval + obj\n",
        "            w = w - learning_rate * g\n",
        "        \n",
        "        learning_rate = learning_rate * 0.9\n",
        "        objval = objval/n\n",
        "        objvals[i] = objval\n",
        "        print('Objective value at epoch ' + str(i+1) + ' is ' + str(objval))\n",
        "    \n",
        "    return w, objvals\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGmdeRGntoOh"
      },
      "source": [
        "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tSQp0SA4toOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef5a151-7cad-436a-9a7d-c3dbb47ec05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at epoch 1 is 0.11324245139722891\n",
            "Objective value at epoch 2 is 0.0728534473423762\n",
            "Objective value at epoch 3 is 0.06490444631443364\n",
            "Objective value at epoch 4 is 0.06180933020744074\n",
            "Objective value at epoch 5 is 0.0594313690783196\n",
            "Objective value at epoch 6 is 0.05870820214340375\n",
            "Objective value at epoch 7 is 0.056890696846319755\n",
            "Objective value at epoch 8 is 0.0559987657844674\n",
            "Objective value at epoch 9 is 0.054949744140390985\n",
            "Objective value at epoch 10 is 0.05408081428929107\n",
            "Objective value at epoch 11 is 0.053689682653091236\n",
            "Objective value at epoch 12 is 0.05253419094947232\n",
            "Objective value at epoch 13 is 0.052895009813742976\n",
            "Objective value at epoch 14 is 0.05222493264001225\n",
            "Objective value at epoch 15 is 0.051716907479316225\n",
            "Objective value at epoch 16 is 0.05159276948588305\n",
            "Objective value at epoch 17 is 0.051264259893958546\n",
            "Objective value at epoch 18 is 0.050975339038356444\n",
            "Objective value at epoch 19 is 0.050723436325672396\n",
            "Objective value at epoch 20 is 0.050557848344156844\n",
            "Objective value at epoch 21 is 0.050348940848532614\n",
            "Objective value at epoch 22 is 0.050172560796212774\n",
            "Objective value at epoch 23 is 0.050050636164226114\n",
            "Objective value at epoch 24 is 0.04991145125166963\n",
            "Objective value at epoch 25 is 0.04978080887941429\n",
            "Objective value at epoch 26 is 0.04961980334568296\n",
            "Objective value at epoch 27 is 0.04960053712813817\n",
            "Objective value at epoch 28 is 0.049509112565590215\n",
            "Objective value at epoch 29 is 0.04943836257509097\n",
            "Objective value at epoch 30 is 0.0493738811310816\n",
            "Objective value at epoch 31 is 0.0493146153840366\n",
            "Objective value at epoch 32 is 0.049256737351053695\n",
            "Objective value at epoch 33 is 0.04920759694220117\n",
            "Objective value at epoch 34 is 0.04916098214690419\n",
            "Objective value at epoch 35 is 0.049120425096941825\n",
            "Objective value at epoch 36 is 0.04908700254586866\n",
            "Objective value at epoch 37 is 0.04905717278089189\n",
            "Objective value at epoch 38 is 0.04902731142836185\n",
            "Objective value at epoch 39 is 0.049000974104623646\n",
            "Objective value at epoch 40 is 0.048977455687923686\n",
            "Objective value at epoch 41 is 0.04895535509446358\n",
            "Objective value at epoch 42 is 0.04893656141648548\n",
            "Objective value at epoch 43 is 0.048919663439250916\n",
            "Objective value at epoch 44 is 0.048904546286431455\n",
            "Objective value at epoch 45 is 0.04889060281779213\n",
            "Objective value at epoch 46 is 0.04887815113619981\n",
            "Objective value at epoch 47 is 0.048866150847253896\n",
            "Objective value at epoch 48 is 0.04885672166861514\n",
            "Objective value at epoch 49 is 0.04884782562721349\n",
            "Objective value at epoch 50 is 0.04883909026515288\n",
            "Objective value at epoch 51 is 0.048832250114978486\n",
            "Objective value at epoch 52 is 0.048825658105367875\n",
            "Objective value at epoch 53 is 0.048819713590727114\n",
            "Objective value at epoch 54 is 0.048814357300647454\n",
            "Objective value at epoch 55 is 0.048809527706475606\n",
            "Objective value at epoch 56 is 0.04880516969983429\n",
            "Objective value at epoch 57 is 0.04880128870205413\n",
            "Objective value at epoch 58 is 0.04879778071106628\n",
            "Objective value at epoch 59 is 0.04879460785969338\n",
            "Objective value at epoch 60 is 0.0487917772526248\n",
            "Objective value at epoch 61 is 0.048789214662997536\n",
            "Objective value at epoch 62 is 0.04878690308275021\n",
            "Objective value at epoch 63 is 0.048784834282501084\n",
            "Objective value at epoch 64 is 0.048782973077184835\n",
            "Objective value at epoch 65 is 0.04878129441738828\n",
            "Objective value at epoch 66 is 0.048779780251333935\n",
            "Objective value at epoch 67 is 0.04877842487530942\n",
            "Objective value at epoch 68 is 0.04877720121340047\n",
            "Objective value at epoch 69 is 0.048776097859150144\n",
            "Objective value at epoch 70 is 0.048775108283393376\n",
            "Objective value at epoch 71 is 0.04877421474326032\n",
            "Objective value at epoch 72 is 0.04877341317732899\n",
            "Objective value at epoch 73 is 0.04877268988940189\n",
            "Objective value at epoch 74 is 0.048772041235771335\n",
            "Objective value at epoch 75 is 0.048771456010493784\n",
            "Objective value at epoch 76 is 0.04877092861715555\n",
            "Objective value at epoch 77 is 0.04877045528367403\n",
            "Objective value at epoch 78 is 0.048770028652991265\n",
            "Objective value at epoch 79 is 0.048769644821969174\n",
            "Objective value at epoch 80 is 0.04876929917320815\n",
            "Objective value at epoch 81 is 0.04876898822499929\n",
            "Objective value at epoch 82 is 0.04876870846790334\n",
            "Objective value at epoch 83 is 0.04876845665340985\n",
            "Objective value at epoch 84 is 0.04876822986590017\n",
            "Objective value at epoch 85 is 0.04876802585514101\n",
            "Objective value at epoch 86 is 0.04876784229054268\n",
            "Objective value at epoch 87 is 0.048767677065268994\n",
            "Objective value at epoch 88 is 0.04876752824388368\n",
            "Objective value at epoch 89 is 0.04876739449682911\n",
            "Objective value at epoch 90 is 0.048767274009551685\n",
            "Objective value at epoch 91 is 0.048767165621674116\n",
            "Objective value at epoch 92 is 0.04876706802970285\n",
            "Objective value at epoch 93 is 0.04876698023196771\n",
            "Objective value at epoch 94 is 0.048766901185233184\n",
            "Objective value at epoch 95 is 0.04876683006082056\n",
            "Objective value at epoch 96 is 0.04876676605014593\n",
            "Objective value at epoch 97 is 0.0487667084334206\n",
            "Objective value at epoch 98 is 0.04876665657661347\n",
            "Objective value at epoch 99 is 0.048766609909900574\n",
            "Objective value at epoch 100 is 0.04876656790689768\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0\n",
        "learning_rate = 0.1\n",
        "w = numpy.zeros((d,1))\n",
        "\n",
        "w_sgd, objvals_sgd = sgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HOzYlf9VtoOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7f08c6-b37f-4485-8744-cb34b0365f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at epoch 1 is 0.11163339211294143\n",
            "Objective value at epoch 2 is 0.07218657716484639\n",
            "Objective value at epoch 3 is 0.06541293950669302\n",
            "Objective value at epoch 4 is 0.06321257488732199\n",
            "Objective value at epoch 5 is 0.06047657145813814\n",
            "Objective value at epoch 6 is 0.05976227869542712\n",
            "Objective value at epoch 7 is 0.05740348104170829\n",
            "Objective value at epoch 8 is 0.056957199834313674\n",
            "Objective value at epoch 9 is 0.05592425606606097\n",
            "Objective value at epoch 10 is 0.055792098138796264\n",
            "Objective value at epoch 11 is 0.05468006825252707\n",
            "Objective value at epoch 12 is 0.05444986334449083\n",
            "Objective value at epoch 13 is 0.053886449742748095\n",
            "Objective value at epoch 14 is 0.05355837274798023\n",
            "Objective value at epoch 15 is 0.053041021629771916\n",
            "Objective value at epoch 16 is 0.052783857580372386\n",
            "Objective value at epoch 17 is 0.05273282857957945\n",
            "Objective value at epoch 18 is 0.05240986463698108\n",
            "Objective value at epoch 19 is 0.05208618467560232\n",
            "Objective value at epoch 20 is 0.051874605438910606\n",
            "Objective value at epoch 21 is 0.05166981291136571\n",
            "Objective value at epoch 22 is 0.0516415521682275\n",
            "Objective value at epoch 23 is 0.051498832008565094\n",
            "Objective value at epoch 24 is 0.05137545859759633\n",
            "Objective value at epoch 25 is 0.05127479826996328\n",
            "Objective value at epoch 26 is 0.05115319168451875\n",
            "Objective value at epoch 27 is 0.05107453930703821\n",
            "Objective value at epoch 28 is 0.05099085786971123\n",
            "Objective value at epoch 29 is 0.05090662583470083\n",
            "Objective value at epoch 30 is 0.0508501909798119\n",
            "Objective value at epoch 31 is 0.05079212768508034\n",
            "Objective value at epoch 32 is 0.05073595442096583\n",
            "Objective value at epoch 33 is 0.050695267847674585\n",
            "Objective value at epoch 34 is 0.050651743669884015\n",
            "Objective value at epoch 35 is 0.05061184464029291\n",
            "Objective value at epoch 36 is 0.05057985470170652\n",
            "Objective value at epoch 37 is 0.05054972600483253\n",
            "Objective value at epoch 38 is 0.050521568464934496\n",
            "Objective value at epoch 39 is 0.05049645577225268\n",
            "Objective value at epoch 40 is 0.05047457789794476\n",
            "Objective value at epoch 41 is 0.05045437286172779\n",
            "Objective value at epoch 42 is 0.05043533849835872\n",
            "Objective value at epoch 43 is 0.0504199130938717\n",
            "Objective value at epoch 44 is 0.050405053981934796\n",
            "Objective value at epoch 45 is 0.05039206437985299\n",
            "Objective value at epoch 46 is 0.050379833345118544\n",
            "Objective value at epoch 47 is 0.05036940517441347\n",
            "Objective value at epoch 48 is 0.05035962767740389\n",
            "Objective value at epoch 49 is 0.050351210948516245\n",
            "Objective value at epoch 50 is 0.050343492485650794\n",
            "Objective value at epoch 51 is 0.05033639394746894\n",
            "Objective value at epoch 52 is 0.05033009030814332\n",
            "Objective value at epoch 53 is 0.05032438112313361\n",
            "Objective value at epoch 54 is 0.05031937297417506\n",
            "Objective value at epoch 55 is 0.050314615659565354\n",
            "Objective value at epoch 56 is 0.0503106292863635\n",
            "Objective value at epoch 57 is 0.050306921517702004\n",
            "Objective value at epoch 58 is 0.05030357268370041\n",
            "Objective value at epoch 59 is 0.05030055090147847\n",
            "Objective value at epoch 60 is 0.050297844096892384\n",
            "Objective value at epoch 61 is 0.050295397757996387\n",
            "Objective value at epoch 62 is 0.050293207197940934\n",
            "Objective value at epoch 63 is 0.0502912379202748\n",
            "Objective value at epoch 64 is 0.050289455548013284\n",
            "Objective value at epoch 65 is 0.050287862522610374\n",
            "Objective value at epoch 66 is 0.05028642120450213\n",
            "Objective value at epoch 67 is 0.05028512725286366\n",
            "Objective value at epoch 68 is 0.05028396008467226\n",
            "Objective value at epoch 69 is 0.050282910104949445\n",
            "Objective value at epoch 70 is 0.05028196651179024\n",
            "Objective value at epoch 71 is 0.05028111580089716\n",
            "Objective value at epoch 72 is 0.05028034970731723\n",
            "Objective value at epoch 73 is 0.05027966131637369\n",
            "Objective value at epoch 74 is 0.050279041398914336\n",
            "Objective value at epoch 75 is 0.050278485129976375\n",
            "Objective value at epoch 76 is 0.050277982991691894\n",
            "Objective value at epoch 77 is 0.05027753165488121\n",
            "Objective value at epoch 78 is 0.05027712476195889\n",
            "Objective value at epoch 79 is 0.0502767589032128\n",
            "Objective value at epoch 80 is 0.05027642987337973\n",
            "Objective value at epoch 81 is 0.050276133562482186\n",
            "Objective value at epoch 82 is 0.05027586688209756\n",
            "Objective value at epoch 83 is 0.05027562686153281\n",
            "Objective value at epoch 84 is 0.05027541079897492\n",
            "Objective value at epoch 85 is 0.05027521642464167\n",
            "Objective value at epoch 86 is 0.05027504142795029\n",
            "Objective value at epoch 87 is 0.05027488392378868\n",
            "Objective value at epoch 88 is 0.050274742107147226\n",
            "Objective value at epoch 89 is 0.050274614625494575\n",
            "Objective value at epoch 90 is 0.050274499834145885\n",
            "Objective value at epoch 91 is 0.05027439647362481\n",
            "Objective value at epoch 92 is 0.05027430352894558\n",
            "Objective value at epoch 93 is 0.05027421983471563\n",
            "Objective value at epoch 94 is 0.05027414450905188\n",
            "Objective value at epoch 95 is 0.050274076717255096\n",
            "Objective value at epoch 96 is 0.05027401571615215\n",
            "Objective value at epoch 97 is 0.0502739608053648\n",
            "Objective value at epoch 98 is 0.05027391138702801\n",
            "Objective value at epoch 99 is 0.05027386691162076\n",
            "Objective value at epoch 100 is 0.05027382688337846\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0.0001\n",
        "learning_rate = 0.1\n",
        "w = numpy.zeros((d,1))\n",
        "\n",
        "w_sgd_regularized, objvals_sgd_regularized = sgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS-UPn9wtoOh"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpnFg1ptoOh"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DlpdmabltoOh"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label:  b-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "    n,d = xi.shape\n",
        "    expTerm = np.dot(yi * xi, w) \n",
        "    \n",
        "    logTerm = np.log(1 + np.exp(-expTerm)) \n",
        "    obj = np.mean(logTerm) + lam/2 * np.sum(w*w) \n",
        "\n",
        "    gTerm = - np.mean(np.divide(yi * xi, 1 + np.exp(expTerm)), axis = 0).reshape(d,1) \n",
        "    g = gTerm + lam * w \n",
        "    \n",
        "    return obj, g\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB74Q4N_toOi"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
        "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "WFosBJ_atoOi"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    n, d = x.shape\n",
        "    batch_size = 10\n",
        "    objvals = numpy.zeros(max_epoch)\n",
        "    \n",
        "    for i in range(max_epoch):\n",
        "        rand_indices = numpy.random.permutation(n)\n",
        "        x_rand = x[rand_indices, :]\n",
        "        y_rand = y[rand_indices, :]\n",
        "        \n",
        "        objval = 0 \n",
        "        for j in range(0, n, batch_size):\n",
        "            xi = x_rand[j:j+batch_size, :] \n",
        "            yi = y_rand[j:j+batch_size, :] \n",
        "            obj, g = mb_objective_gradient(w, xi, yi, lam)\n",
        "            objval = objval + obj\n",
        "            w = w - learning_rate * g\n",
        "        \n",
        "        learning_rate = learning_rate * 0.9 \n",
        "        objval /= n/batch_size\n",
        "        objvals[i] = objval\n",
        "        print('Objective value at epoch ' + str(i+1) + ' is ' + str(objval))\n",
        "    \n",
        "    return w, objvals\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z_DAIqEtoOi"
      },
      "source": [
        "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kRvz8aYCtoOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84365cfa-45a5-4d1e-a8d2-c1eef95a8126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at epoch 1 is 0.14165592533368002\n",
            "Objective value at epoch 2 is 0.07397264716433943\n",
            "Objective value at epoch 3 is 0.06694274561640264\n",
            "Objective value at epoch 4 is 0.06270078140419523\n",
            "Objective value at epoch 5 is 0.06132557276989168\n",
            "Objective value at epoch 6 is 0.059387756202411476\n",
            "Objective value at epoch 7 is 0.05762534832698657\n",
            "Objective value at epoch 8 is 0.05611777567848969\n",
            "Objective value at epoch 9 is 0.05548675778415022\n",
            "Objective value at epoch 10 is 0.05430036897183852\n",
            "Objective value at epoch 11 is 0.053980227211386955\n",
            "Objective value at epoch 12 is 0.053303361545874436\n",
            "Objective value at epoch 13 is 0.052741221062820906\n",
            "Objective value at epoch 14 is 0.05246055070451647\n",
            "Objective value at epoch 15 is 0.05166977162888939\n",
            "Objective value at epoch 16 is 0.05153377474602657\n",
            "Objective value at epoch 17 is 0.05137573941784474\n",
            "Objective value at epoch 18 is 0.0513029920987331\n",
            "Objective value at epoch 19 is 0.0514002228647327\n",
            "Objective value at epoch 20 is 0.050826109367238644\n",
            "Objective value at epoch 21 is 0.05111346940314851\n",
            "Objective value at epoch 22 is 0.05037489422614255\n",
            "Objective value at epoch 23 is 0.05015912197280426\n",
            "Objective value at epoch 24 is 0.05029788094715462\n",
            "Objective value at epoch 25 is 0.049897390379683944\n",
            "Objective value at epoch 26 is 0.049840348689839456\n",
            "Objective value at epoch 27 is 0.04986708019141873\n",
            "Objective value at epoch 28 is 0.05102430733026342\n",
            "Objective value at epoch 29 is 0.04998002370792754\n",
            "Objective value at epoch 30 is 0.049483274230079075\n",
            "Objective value at epoch 31 is 0.04946391409003943\n",
            "Objective value at epoch 32 is 0.049462181570685715\n",
            "Objective value at epoch 33 is 0.049385590554114206\n",
            "Objective value at epoch 34 is 0.04949310762541765\n",
            "Objective value at epoch 35 is 0.04929356494209569\n",
            "Objective value at epoch 36 is 0.049304987397046\n",
            "Objective value at epoch 37 is 0.049203296550856784\n",
            "Objective value at epoch 38 is 0.049187303700337665\n",
            "Objective value at epoch 39 is 0.049840008124762365\n",
            "Objective value at epoch 40 is 0.0491764468682601\n",
            "Objective value at epoch 41 is 0.04910605547942119\n",
            "Objective value at epoch 42 is 0.04920446021395366\n",
            "Objective value at epoch 43 is 0.04908752156965363\n",
            "Objective value at epoch 44 is 0.049363942896064904\n",
            "Objective value at epoch 45 is 0.049395322918569844\n",
            "Objective value at epoch 46 is 0.04909703760514352\n",
            "Objective value at epoch 47 is 0.05049086498654031\n",
            "Objective value at epoch 48 is 0.04901264624779596\n",
            "Objective value at epoch 49 is 0.04907819985251288\n",
            "Objective value at epoch 50 is 0.049005426169697795\n",
            "Objective value at epoch 51 is 0.04959843876335226\n",
            "Objective value at epoch 52 is 0.048977405214347285\n",
            "Objective value at epoch 53 is 0.05338545714353336\n",
            "Objective value at epoch 54 is 0.04897107657968391\n",
            "Objective value at epoch 55 is 0.04985475858808611\n",
            "Objective value at epoch 56 is 0.04898772764441925\n",
            "Objective value at epoch 57 is 0.04898958693677177\n",
            "Objective value at epoch 58 is 0.050074914863409974\n",
            "Objective value at epoch 59 is 0.049444106452614704\n",
            "Objective value at epoch 60 is 0.049198480067041986\n",
            "Objective value at epoch 61 is 0.04916591265969983\n",
            "Objective value at epoch 62 is 0.048996174099868076\n",
            "Objective value at epoch 63 is 0.04900710639952061\n",
            "Objective value at epoch 64 is 0.04921681381064457\n",
            "Objective value at epoch 65 is 0.04981426701460646\n",
            "Objective value at epoch 66 is 0.04960258733798779\n",
            "Objective value at epoch 67 is 0.04960192738215645\n",
            "Objective value at epoch 68 is 0.04906411200940694\n",
            "Objective value at epoch 69 is 0.049236295106204295\n",
            "Objective value at epoch 70 is 0.04909136486971314\n",
            "Objective value at epoch 71 is 0.04893845145841974\n",
            "Objective value at epoch 72 is 0.04892892451180057\n",
            "Objective value at epoch 73 is 0.04896730212193828\n",
            "Objective value at epoch 74 is 0.04893116042212556\n",
            "Objective value at epoch 75 is 0.04892620760863803\n",
            "Objective value at epoch 76 is 0.04895983719828275\n",
            "Objective value at epoch 77 is 0.050150454156486696\n",
            "Objective value at epoch 78 is 0.048926838436303806\n",
            "Objective value at epoch 79 is 0.05023402766431346\n",
            "Objective value at epoch 80 is 0.04900077600775069\n",
            "Objective value at epoch 81 is 0.04894495053888864\n",
            "Objective value at epoch 82 is 0.04936057793531954\n",
            "Objective value at epoch 83 is 0.048956662615805314\n",
            "Objective value at epoch 84 is 0.049083887757908055\n",
            "Objective value at epoch 85 is 0.048930515687824624\n",
            "Objective value at epoch 86 is 0.04900310202761488\n",
            "Objective value at epoch 87 is 0.0496523434937738\n",
            "Objective value at epoch 88 is 0.05037838267540435\n",
            "Objective value at epoch 89 is 0.048989290083839995\n",
            "Objective value at epoch 90 is 0.048926884584473154\n",
            "Objective value at epoch 91 is 0.04900557938595357\n",
            "Objective value at epoch 92 is 0.04896100678467317\n",
            "Objective value at epoch 93 is 0.04923028990151736\n",
            "Objective value at epoch 94 is 0.05789404256553939\n",
            "Objective value at epoch 95 is 0.048953338280483844\n",
            "Objective value at epoch 96 is 0.05051212539288143\n",
            "Objective value at epoch 97 is 0.04910432464310061\n",
            "Objective value at epoch 98 is 0.049082142919852155\n",
            "Objective value at epoch 99 is 0.048934631768693324\n",
            "Objective value at epoch 100 is 0.04892249574973536\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0\n",
        "learning_rate = 1\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_mbgd, objvals_mbgd = mbgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jp6sQsMHtoOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d7eaf0-bd30-423b-90b1-f9f0b0ae5f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective value at epoch 1 is 0.10751298724933993\n",
            "Objective value at epoch 2 is 0.071912381853286\n",
            "Objective value at epoch 3 is 0.0636318340793177\n",
            "Objective value at epoch 4 is 0.06341312606296094\n",
            "Objective value at epoch 5 is 0.06117232658365274\n",
            "Objective value at epoch 6 is 0.05983695982651986\n",
            "Objective value at epoch 7 is 0.056970336955482426\n",
            "Objective value at epoch 8 is 0.05643061984550533\n",
            "Objective value at epoch 9 is 0.05634220883887406\n",
            "Objective value at epoch 10 is 0.05540613889915087\n",
            "Objective value at epoch 11 is 0.05584173891213767\n",
            "Objective value at epoch 12 is 0.05441851257246285\n",
            "Objective value at epoch 13 is 0.05395284699087037\n",
            "Objective value at epoch 14 is 0.05325308778676249\n",
            "Objective value at epoch 15 is 0.052965480175561075\n",
            "Objective value at epoch 16 is 0.052717192324914244\n",
            "Objective value at epoch 17 is 0.0525013848333102\n",
            "Objective value at epoch 18 is 0.05234308794155378\n",
            "Objective value at epoch 19 is 0.0531222545228306\n",
            "Objective value at epoch 20 is 0.05205859537601768\n",
            "Objective value at epoch 21 is 0.05154685469970097\n",
            "Objective value at epoch 22 is 0.051628504390246216\n",
            "Objective value at epoch 23 is 0.05249539037075901\n",
            "Objective value at epoch 24 is 0.05184193902916748\n",
            "Objective value at epoch 25 is 0.05570981696989974\n",
            "Objective value at epoch 26 is 0.05123086020234428\n",
            "Objective value at epoch 27 is 0.05111089376358879\n",
            "Objective value at epoch 28 is 0.0511729812683576\n",
            "Objective value at epoch 29 is 0.050830732882691625\n",
            "Objective value at epoch 30 is 0.050758367413135136\n",
            "Objective value at epoch 31 is 0.05085646750366237\n",
            "Objective value at epoch 32 is 0.05109063460320183\n",
            "Objective value at epoch 33 is 0.05495023764644455\n",
            "Objective value at epoch 34 is 0.05137618093345772\n",
            "Objective value at epoch 35 is 0.050557066229937025\n",
            "Objective value at epoch 36 is 0.05047049751451562\n",
            "Objective value at epoch 37 is 0.0504622656359001\n",
            "Objective value at epoch 38 is 0.05043517684187176\n",
            "Objective value at epoch 39 is 0.05197559641140839\n",
            "Objective value at epoch 40 is 0.05041055200207826\n",
            "Objective value at epoch 41 is 0.052557072649597594\n",
            "Objective value at epoch 42 is 0.05034371390002748\n",
            "Objective value at epoch 43 is 0.050486816204819325\n",
            "Objective value at epoch 44 is 0.05041452924768262\n",
            "Objective value at epoch 45 is 0.0502926634889704\n",
            "Objective value at epoch 46 is 0.0507443673122029\n",
            "Objective value at epoch 47 is 0.05063144689555787\n",
            "Objective value at epoch 48 is 0.05028670614549694\n",
            "Objective value at epoch 49 is 0.05026664455165826\n",
            "Objective value at epoch 50 is 0.05024881061882042\n",
            "Objective value at epoch 51 is 0.051078692670410146\n",
            "Objective value at epoch 52 is 0.050380945101515036\n",
            "Objective value at epoch 53 is 0.050246475983251875\n",
            "Objective value at epoch 54 is 0.05023498256837746\n",
            "Objective value at epoch 55 is 0.050274753191153515\n",
            "Objective value at epoch 56 is 0.050354122939239084\n",
            "Objective value at epoch 57 is 0.0503882721760971\n",
            "Objective value at epoch 58 is 0.050249594511168644\n",
            "Objective value at epoch 59 is 0.05119046111807906\n",
            "Objective value at epoch 60 is 0.05141500165931137\n",
            "Objective value at epoch 61 is 0.05038813210903004\n",
            "Objective value at epoch 62 is 0.05041265110519383\n",
            "Objective value at epoch 63 is 0.05063993448374462\n",
            "Objective value at epoch 64 is 0.05447594701614296\n",
            "Objective value at epoch 65 is 0.050193001010461855\n",
            "Objective value at epoch 66 is 0.05170393006221559\n",
            "Objective value at epoch 67 is 0.0502454187879954\n",
            "Objective value at epoch 68 is 0.050216244741928935\n",
            "Objective value at epoch 69 is 0.050241759549381905\n",
            "Objective value at epoch 70 is 0.05026275392087685\n",
            "Objective value at epoch 71 is 0.05018825524726693\n",
            "Objective value at epoch 72 is 0.05028183824472278\n",
            "Objective value at epoch 73 is 0.050658779738202216\n",
            "Objective value at epoch 74 is 0.05018790741008801\n",
            "Objective value at epoch 75 is 0.050614418537020575\n",
            "Objective value at epoch 76 is 0.050184238884872584\n",
            "Objective value at epoch 77 is 0.050691786011658774\n",
            "Objective value at epoch 78 is 0.050455938762191566\n",
            "Objective value at epoch 79 is 0.05087503684686359\n",
            "Objective value at epoch 80 is 0.0507210038200548\n",
            "Objective value at epoch 81 is 0.05022628494317884\n",
            "Objective value at epoch 82 is 0.050598695241284616\n",
            "Objective value at epoch 83 is 0.05019013182053712\n",
            "Objective value at epoch 84 is 0.05018102712778049\n",
            "Objective value at epoch 85 is 0.05031279387399764\n",
            "Objective value at epoch 86 is 0.05037480631189752\n",
            "Objective value at epoch 87 is 0.05051068383460989\n",
            "Objective value at epoch 88 is 0.050652131793668044\n",
            "Objective value at epoch 89 is 0.0508591107734808\n",
            "Objective value at epoch 90 is 0.05076314828387841\n",
            "Objective value at epoch 91 is 0.05033880934467589\n",
            "Objective value at epoch 92 is 0.05037639183969395\n",
            "Objective value at epoch 93 is 0.05114752414666884\n",
            "Objective value at epoch 94 is 0.051786865312522086\n",
            "Objective value at epoch 95 is 0.05036358542367062\n",
            "Objective value at epoch 96 is 0.0502009684731861\n",
            "Objective value at epoch 97 is 0.05022192949627894\n",
            "Objective value at epoch 98 is 0.050351838338030766\n",
            "Objective value at epoch 99 is 0.05035491561958433\n",
            "Objective value at epoch 100 is 0.05020520352624353\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0.0001\n",
        "learning_rate = 1\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_mbgd_regularized, objvals_mbgd_regularized = mbgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0YDQ0vZtoOi"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "xL58mvsktoOi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "1bf7334d-0c9a-46d9-cf8c-a54777b0110b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "line1, = plt.plot(range(len(objvals_gd)), objvals_gd, 'y')\n",
        "line2, = plt.plot(range(len(objvals_sgd)), objvals_sgd, '-.r')\n",
        "line3, = plt.plot(range(len(objvals_mbgd)), objvals_mbgd, '--g')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Objective Value')\n",
        "plt.legend([line1, line2, line3], ['GD', 'SGD', 'MBGD'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "line4, = plt.plot(range(len(objvals_gd_regularized)), objvals_gd_regularized, 'y')\n",
        "line5, = plt.plot(range(len(objvals_sgd_regularized)), objvals_sgd_regularized, '-.r')\n",
        "line6, = plt.plot(range(len(objvals_mbgd_regularized)), objvals_mbgd_regularized, '--g')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Objective Value')\n",
        "plt.legend([line4, line5, line6], ['GD_Regularized', 'SGD_Regularized', 'MBGD_Regularized'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "-xswUV2rkb_I",
        "outputId": "f811ad8a-a86d-4ea9-a0ed-b5bac4708db4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnbk5XVtoOi"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Il6wb2GdtoOi"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "def predict(w, X):\n",
        "    xw = numpy.dot(X, w)\n",
        "    f = numpy.sign(xw)\n",
        "    return f\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Gradient Descent"
      ],
      "metadata": {
        "id": "B1MAwXCPLUD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8MnUL1UbtoOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe36e1e-118e-41b2-ce45-5a8d284e296c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.01098901098901099\n",
            "Test classification error is 0.043859649122807015\n"
          ]
        }
      ],
      "source": [
        "f_train = predict(w_gd, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_gd, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Gradient Descent Regularized"
      ],
      "metadata": {
        "id": "qhNFrNxjLlvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train = predict(w_gd_regularized, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_gd_regularized, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S__uC4OC5diT",
        "outputId": "a7ec6aff-6001-4e11-8c51-ee45cf5d11c3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.01098901098901099\n",
            "Test classification error is 0.043859649122807015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "cFyHmCjiLx5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train = predict(w_sgd, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_sgd, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYaHpGtt5d7z",
        "outputId": "a5943a8e-5615-49dd-95f1-8fdbf27ec971"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.008791208791208791\n",
            "Test classification error is 0.03508771929824561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Stochastic Gradient Descent Regularized"
      ],
      "metadata": {
        "id": "4QADIUxLL0_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TEhT-VGGtoOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2974a7-1a34-4aef-b503-c4981fb09385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.013186813186813187\n",
            "Test classification error is 0.043859649122807015\n"
          ]
        }
      ],
      "source": [
        "f_train = predict(w_sgd_regularized, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_sgd_regularized, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "pf-JovdXL19T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train = predict(w_mbgd, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_mbgd, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hUL32w65g1_",
        "outputId": "9c159658-5909-4917-f5c3-4d57dc298e61"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.01098901098901099\n",
            "Test classification error is 0.03508771929824561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing classification error of Mini-Batch Gradient Descent Regularized"
      ],
      "metadata": {
        "id": "rTJPsBtmL2x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train = predict(w_mbgd_regularized, x_train)\n",
        "diff = numpy.abs(f_train - y_train) / 2\n",
        "error_train = numpy.mean(diff)\n",
        "print('Training classification error is ' + str(error_train))\n",
        "\n",
        "f_test = predict(w_mbgd_regularized, x_test)\n",
        "diff = numpy.abs(f_test - y_test) / 2\n",
        "error_test = numpy.mean(diff)\n",
        "print('Test classification error is ' + str(error_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVR0wn_H5g7T",
        "outputId": "c2b33188-0171-43f8-d3ab-c28b9978424f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error is 0.008791208791208791\n",
            "Test classification error is 0.03508771929824561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC6hCw8XtoOi"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu5CZC1KtoOi"
      },
      "source": [
        "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "YsvXNgpVtoOi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assignment_1_Q5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}